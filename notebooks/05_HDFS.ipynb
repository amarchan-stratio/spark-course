{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e226212a-5c9e-4933-9a72-cacef8b9adca",
   "metadata": {},
   "source": [
    "# HDFS (Hadoop Distributed File System)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6167be28-c8f8-4251-aada-b0da52a3a610",
   "metadata": {},
   "source": [
    "## What is HDFS?\n",
    "\n",
    "HDFS, or the Hadoop Distributed File System, is the primary storage system of Hadoop, which is an open-source framework for processing and storing large datasets in a distributed computing environment. HDFS is designed to scale up from a single server to thousands of machines, with each offering local computation and storage.\n",
    "\n",
    "**Key features**:\n",
    "1. **Block-based Structure**: Data in HDFS is stored in blocks (commonly 128 MB or 256 MB in size), and these blocks are distributed across the cluster. Each block is replicated multiple times (usually three) to handle hardware failure.\n",
    "\n",
    "2. **Fault Tolerance**: Due to its block replication mechanism, HDFS is fault-tolerant. If a block or node fails, data can be recovered from another node where the block is replicated.\n",
    "\n",
    "3. **Write-once, Read-many Model**: HDFS is primarily designed for large data sets and supports a write-once and read-many times paradigm. This model simplifies data coherency issues.\n",
    "\n",
    "4. **High Throughput & Scalability**: HDFS is designed to provide high throughput for data access and can easily scale out by adding more machines to the cluster.\n",
    "\n",
    "5. **Data Locality**: One of the primary objectives of HDFS is to store data on the compute nodes so that processing tasks can run on nodes where data is locally stored. This minimizes the data transfer across the cluster and increases processing speed.\n",
    "\n",
    "6. **Simple Coherency Model**: Once written, data/files can't be modified, only appended to. This eliminates potential issues that can arise from multiple sources trying to update a file simultaneously.\n",
    "\n",
    "7. **Large Data Sets**: HDFS is designed to handle very large files, making it suitable for big data processing tasks.\n",
    "\n",
    "8. **Streaming Data Access**: HDFS is optimized for streaming access of its datasets, meaning it's best suited for applications that require sequential access, rather than random access.\n",
    "\n",
    "9. **Integration with Hadoop Ecosystem**: HDFS is deeply integrated with various components of the Hadoop ecosystem like MapReduce, YARN, Hive, Pig, and others. This allows for efficient processing and management of big data.\n",
    "\n",
    "To sum it up, HDFS is a distributed and scalable file system that is a fundamental component of the Hadoop ecosystem and is designed specifically for storing and processing massive datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2571fff1-b837-4873-b6ef-b6bc96eca919",
   "metadata": {},
   "source": [
    "## HDFS architecture\n",
    "\n",
    "<img src=\"images/hdfs_architecture.png\" title=\"HDFS Architecture\" width=\"700px\"/>\n",
    "\n",
    "The HDFS architecture is designed with a master-slave topology, where data is broken into blocks and distributed across multiple nodes in a cluster. Let's dive into the primary components and their roles:\n",
    "\n",
    "**1. NameNode (Master Server)**:\n",
    "   - **Role**: Manages and maintains the metadata of HDFS.\n",
    "   - **Function**: Does not store the actual data but maintains the file system tree and the metadata for all the files and directories in the system. This metadata is stored in RAM for fast access.\n",
    "   - **Responsibilities**: \n",
    "     - Managing the file system namespace.\n",
    "     - Regulating client access to files.\n",
    "     - Executing file system operations such as renaming, closing, and opening files and directories.\n",
    "     - Keeping track of the block mapping to DataNodes.\n",
    "\n",
    "**2. Secondary NameNode**:\n",
    "   - **Role**: Performs housekeeping functions for the NameNode.\n",
    "   - **Function**: It periodically merges the changes (edits) with the filesystem image (fsimage) and produces an updated version of fsimage. This helps in preventing the edit log on the NameNode from becoming excessively large.\n",
    "   - **Note**: Many modern Hadoop deployments use the HA (High Availability) architecture, replacing the traditional Secondary NameNode with a Standby NameNode.\n",
    "\n",
    "**3. DataNode (Slave Server)**:\n",
    "   - **Role**: Stores and manages the actual data blocks of HDFS.\n",
    "   - **Function**: \n",
    "     - Stores data in the local file system (like ext4, xfs).\n",
    "     - Creates, deletes, and replicates blocks based on instructions from the NameNode.\n",
    "     - Periodically sends a heartbeat to the NameNode to signal it's alive. Along with the heartbeat, it sends a block report, which lists all the blocks on a DataNode.\n",
    "   \n",
    "**4. Block**:\n",
    "   - **Role**: The fundamental storage unit of HDFS.\n",
    "   - **Function**: Each file is divided into blocks of a fixed size (default is 128MB or 256MB). These blocks are distributed across the cluster, and multiple copies (replicas) of each block are maintained to ensure fault tolerance.\n",
    "   - **Note**: While not a component in the traditional sense like NameNode or DataNode, blocks are a foundational concept in HDFS architecture.\n",
    "\n",
    "**5. Client**:\n",
    "   - **Role**: Interacts with HDFS.\n",
    "   - **Function**: When an HDFS client wants to read a file, it communicates with the NameNode to determine the block locations. The client then contacts the respective DataNodes to read or write data.\n",
    "\n",
    "**6. Cluster**:\n",
    "   - **Role**: A collection of nodes.\n",
    "   - **Function**: A cluster typically consists of a single NameNode (or two in an HA setup) and multiple DataNodes. The client applications can be run on an external machine or on nodes within the cluster.\n",
    "\n",
    "In addition to these core components, the HDFS architecture has built-in features to handle failures and ensure high data availability:\n",
    "\n",
    "- **Replication**: Each block is replicated multiple times (default is three) across different DataNodes to ensure fault tolerance. If a block (or DataNode) fails, data can be read from another replica.\n",
    "\n",
    "- **Fault Tolerance**: HDFS is designed to detect failures and automatically recover from them. If a DataNode fails, the system ensures that the replication factor of all blocks stored on that node is maintained by creating new replicas on other nodes.\n",
    "\n",
    "- **High Availability (HA)**: In HA-enabled HDFS architectures, there are two NameNodes: Active and Standby. Both maintain the file system metadata in sync. If the active NameNode fails, the standby takes over its duties to ensure high availability.\n",
    "\n",
    "This architecture enables HDFS to provide a robust and scalable storage solution suitable for storing vast amounts of data and serving large-scale data processing tasks.\n",
    "\n",
    "## HDFS High Availability architecture (the one used in Stratio)\n",
    "\n",
    "<img src=\"images/hdfs-HA-architecture.png\" title=\"HDFS HA Architecture\" width=\"700px\"/>\n",
    "\n",
    "In addition to all the mentioned components, in HA architectures (like the one used in Stratio) there are some extra components worth mentioning\n",
    "\n",
    "**1. Standby NameNode**: Replaces the traditional Secondary NameNode. It performs the same functions as the Secondary NameNode while acting as a backup to the Active NameNode, prepared to take over its functions without any loss of data or significant downtime in case of failure (Readiness for Failover).\n",
    "\n",
    "**2. JournalNode**:\n",
    "\n",
    "- **Role**:\n",
    "JournalNodes facilitate the High Availability feature of HDFS by providing a way to synchronize metadata changes between the Active and Standby NameNodes.\n",
    "\n",
    "- **Function**:\n",
    "    - JournalNodes maintain a log (or journal) of metadata changes made by the Active NameNode. When the Active NameNode makes any change to the metadata, it records this change in its local logs and also writes it to a majority of the configured JournalNodes.\n",
    "\n",
    "    - The Standby NameNode is continuously watching these JournalNodes and reading the metadata updates. Once it reads these updates, the Standby NameNode applies them to its own namespace, thereby ensuring that both NameNodes remain synchronized.\n",
    "\n",
    "- **Responsibilities**:\n",
    "    1. **Storing Metadata Updates**: JournalNodes receive and store updates from the Active NameNode. These updates include operations like file creation, deletion, renaming, and more.\n",
    "\n",
    "    2. **Synchronization**: Facilitate the synchronization of the metadata changes between the Active and Standby NameNodes, ensuring the Standby NameNode can quickly take over in case of a failure.\n",
    "\n",
    "    3. **Maintaining Write Ahead Logs (WAL)**: Just like databases use Write Ahead Logging for durability and recovery, the JournalNode stores changes in a similar manner. If a NameNode crashes, this log ensures that the state can be fully recovered.\n",
    "\n",
    "    4. **Handling Failovers**: During a failover event (e.g., if the Active NameNode crashes), the Standby NameNode will ensure it has read all the logs from the JournalNodes before promoting itself to the Active state.\n",
    "\n",
    "    5. **Responding to NameNode Requests**: JournalNodes cater to read requests from the Standby NameNode and write requests from the Active NameNode. They ensure the logs are available for reading and store new logs reliably.\n",
    "\n",
    "    6. **Quorum-based Commit**: For a metadata change to be considered committed, it must be written to a majority of JournalNodes (e.g., 2 out of 3, or 3 out of 5). This quorum-based approach ensures that even if a JournalNode or two become unavailable, the system can still function and maintain consistency.\n",
    "\n",
    "**3. Apache ZooKeeper**: is a distributed coordination service. It helps in managing and coordinating the two NameNodes, maintaining configuration information, naming, providing distributed synchronization, and group services. In essence, controls that the system can automatically recover from NameNode failures, ensuring high availability and preventing split-brain scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5826484-6c52-4a82-a53b-c88d14c16bfe",
   "metadata": {},
   "source": [
    "## HDFS useful commands\n",
    "\n",
    "### List Files/Directories\n",
    "   - Command: `hdfs dfs -ls <path>`\n",
    "   - Example: `hdfs dfs -ls /user/hadoop/dir1`\n",
    "\n",
    "### Create a Directory\n",
    "   - Command: `hdfs dfs -mkdir <path>`\n",
    "   - Example: `hdfs dfs -mkdir /user/hadoop/newdir`\n",
    "\n",
    "### Delete a File/Directory\n",
    "   - Command: `hdfs dfs -rm <path>`\n",
    "   - Example: `hdfs dfs -rm /user/hadoop/file1.txt`\n",
    "   - For directories (recursive delete): `hdfs dfs -rm -r /user/hadoop/dir1`\n",
    "\n",
    "### Upload a File to HDFS\n",
    "   - Command: `hdfs dfs -put <local-source> <hdfs-destination>`\n",
    "   - Example: `hdfs dfs -put /localpath/file.txt /user/hadoop/`\n",
    "\n",
    "### Download a File from HDFS\n",
    "   - Command: `hdfs dfs -get <hdfs-source> <local-destination>`\n",
    "   - Example: `hdfs dfs -get /user/hadoop/file.txt /localpath/`\n",
    "\n",
    "### Display File Content\n",
    "   - Command: `hdfs dfs -cat <path>`\n",
    "   - Example: `hdfs dfs -cat /user/hadoop/file1.txt`\n",
    "\n",
    "### Move File/Directory within HDFS\n",
    "   - Command: `hdfs dfs -mv <source> <destination>`\n",
    "   - Example: `hdfs dfs -mv /user/hadoop/file1.txt /user/hadoop/dir1/`\n",
    "\n",
    "### Copy File/Directory within HDFS\n",
    "   - Command: `hdfs dfs -cp <source> <destination>`\n",
    "   - Example: `hdfs dfs -cp /user/hadoop/file1.txt /user/hadoop/dir1/`\n",
    "\n",
    "### Display Disk Usage Statistics\n",
    "   - Command: `hdfs dfs -du <path>`\n",
    "   - Example: `hdfs dfs -du /user/hadoop/`\n",
    "\n",
    "`NOTE`: Add the parameter `-h` for human readable size, which means using MB, GB and not bytes.\n",
    "\n",
    "### Change Owner of a File/Directory\n",
    "   - Command: `hdfs dfs -chown <owner>:<group> <path>`\n",
    "   - Example: `hdfs dfs -chown hadoop:admin /user/hadoop/file1.txt`\n",
    "\n",
    "### Change Permissions of a File/Directory\n",
    "   - Command: `hdfs dfs -chmod <mode> <path>`\n",
    "   - Example: `hdfs dfs -chmod 755 /user/hadoop/file1.txt`\n",
    "\n",
    "### Get Detailed Information about a File/Directory\n",
    "   - Command: `hdfs dfs -stat <path>`\n",
    "   - Example: `hdfs dfs -stat /user/hadoop/file1.txt`\n",
    "\n",
    "### Read the first N lines of a file\n",
    "   - Command: `hdfs dfs -cat <path> | head -n <N_LINES>`\n",
    "   - Example: `hdfs dfs -cat /user/hadoop/file1.txt | head -n 10`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
