{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db7e528f-9ced-4755-b0fb-7e40b2e3ee8c",
   "metadata": {},
   "source": [
    "# Spark Performance Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8678afa7-60cf-4e54-956f-8ca33740f8e9",
   "metadata": {},
   "source": [
    "## 1. Caching and Persistence\n",
    "\n",
    "Caching and persistence are optimization techniques for iterative and interactive Spark computations. They help store intermediate data in memory or more durable storage mediums to avoid recomputing the same information all the time.\n",
    "\n",
    "### 1.1 Why and When to Cache or Persist?\n",
    "\n",
    "Caching in Spark is a way to store computed datasets in memory for subsequent uses without having to recompute them. Using cache effectively can greatly improve the performance of Spark applications. Here are situations where caching is typically beneficial:\n",
    "\n",
    "1. **Iterative Algorithms**: Caching is particularly useful for iterative algorithms like those found in machine learning (e.g., gradient descent in linear regression, iterative computations in graph processing). In each iteration, the same data is processed repeatedly. By caching the dataset, you avoid recomputing or re-reading it in each iteration.\n",
    "\n",
    "2. **Reused Data**: If any DataFrame or RDD is used across multiple Spark operations or actions, it's beneficial to cache it. For example, if you're performing multiple separate aggregations on the same dataset, caching can help.\n",
    "\n",
    "3. **Expensive Computations**: If generating a DataFrame or RDD involves heavy computations or requires data from various sources, caching it once it's computed can save significant time.\n",
    "\n",
    "4. **Frequent Joins with a Small DataFrame**: If you have a smaller DataFrame that gets frequently joined with other larger DataFrames, it might be a good idea to cache the smaller one. In some cases, broadcasting the smaller DataFrame (which inherently caches it on each executor) can be even more beneficial.\n",
    "\n",
    "5. **Interactive Analysis**: When doing exploratory data analysis using tools like SparkSQL or Databricks notebooks, users often run multiple ad-hoc queries on the same dataset. Caching can help speed up this interactive analysis.\n",
    "\n",
    "6. **Checkpointing**: If you're breaking lineage (chain of transformations) to truncate the long lineage or to manage recomputation cost, caching the DataFrame before calling a checkpoint can be advantageous.\n",
    "\n",
    "However, while caching can be very beneficial in these situations, there are times when it may not be optimal:\n",
    "\n",
    "- **Infrequent Access**: If the data is accessed only once, caching might not only be unnecessary but can also add overhead. It might be slower to cache and then compute rather than just compute directly.\n",
    "\n",
    "- **Limited Memory**: If the available memory is limited, caching large DataFrames might cause other cached DataFrames to be evicted or even lead to out-of-memory errors. You need to be cautious about what you choose to cache in such environments.\n",
    "\n",
    "- **Mutable Workloads**: If you keep updating or changing the data, caching may not be efficient as you'll have to persist the changes, which can be expensive.\n",
    "\n",
    "It's essential to monitor the performance and memory usage of your Spark application (using Spark's web UI or other monitoring tools) to ensure that caching is providing the intended benefits. If not used judiciously, caching can lead to memory issues or even degrade performance.\n",
    "\n",
    "### 1.2 Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8dd30b-a22c-489c-a001-132762e58a2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import mean, when, col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark Performance Tuning\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e43e78-5270-43d3-ab4d-e1c42d6bca9e",
   "metadata": {},
   "source": [
    "Lets create a big DataFrame to appreciate caching benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a56707-b761-4e95-a667-0b02af891b95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(\"datasets/students.json\")\n",
    "big_df = df\n",
    "\n",
    "for _ in range(99):\n",
    "    big_df = big_df.union(df)\n",
    "    \n",
    "print(f\"The new DataFrame has {big_df.count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3da652-fcec-489c-bc1a-2446bbc707ae",
   "metadata": {},
   "source": [
    "Let's cache an aggregation transformation on this big DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eab094-4270-4d61-b55c-4dbe4eac6922",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_df = big_df.groupBy(\"age\") \\\n",
    "                    .agg(mean(\"grade\").alias(\"mean_grade\")) \\\n",
    "                    .orderBy(\"age\")\n",
    "cached_df.cache() # Cache the DataFrame \n",
    "# Everytime we access the DataFrame from now on it won't be calculated again\n",
    "cached_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a16347-f037-4c6f-a6b4-c3df0d9b56e2",
   "metadata": {},
   "source": [
    "### 1.3 Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cdba01-9c49-48da-a327-f36fea6b73fd",
   "metadata": {},
   "source": [
    "**Compare the grade of each student to the mean of its age group**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce9d57c-6f08-47b3-ae62-972269a3a8ec",
   "metadata": {},
   "source": [
    "**Cached DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a7a2ea-143b-4a1f-b989-bc35e24fc28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# The expensive_computation_df is not recomputed here as it has been cached\n",
    "big_df.join(cached_df, on=\"age\", how=\"left\") \\\n",
    "    .withColumn(\"performance\", \n",
    "        when(col('grade') > col('mean_grade'), 'Over mean') \\\n",
    "        .otherwise(\n",
    "            when(col('grade') == col('mean_grade'), 'On mean') \\\n",
    "            .otherwise('Under mean')\n",
    "        )\n",
    "    ) \\\n",
    "    .select(\"age\", \"name\", \"surname\", \"performance\") \\\n",
    "    .show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73878e79-4695-4d0b-819a-05485097bad2",
   "metadata": {},
   "source": [
    "To remove a DataFrame from the cache you can call the `unpersist()` function, if you want to remove all the cached DataFrames at once you can call `spark.catalog.clearCache()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddfb9ed-8930-4a1f-891b-8900101e6168",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove the computed DataFrame from the cache\n",
    "cached_df.unpersist()\n",
    "# This removes all the cached DataFrames for the Spark Session\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f07dce-0e0d-4e8f-b6af-ab0f71b04604",
   "metadata": {},
   "source": [
    "**Unpersisted DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a8b37d-233c-4ce4-bc39-024b911c4cfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "big_df.join(cached_df, on=\"age\", how=\"left\") \\\n",
    "    .withColumn(\"performance\", \n",
    "        when(col('grade') > col('mean_grade'), 'Over mean') \\\n",
    "        .otherwise(\n",
    "            when(col('grade') == col('mean_grade'), 'On mean') \\\n",
    "            .otherwise('Under mean')\n",
    "        )\n",
    "    ) \\\n",
    "    .select(\"age\", \"name\", \"surname\", \"performance\") \\\n",
    "    .show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322aba51-2b55-4d26-9e98-607db52269a3",
   "metadata": {},
   "source": [
    "### 1.4 Storage levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a5b751-0ae1-44dd-b01c-9c92b0aa112d",
   "metadata": {},
   "source": [
    "There are the following storage levels available sorted by fastest but smallest to slowest but largest:\n",
    "\n",
    "- MEMORY_ONLY\n",
    "- MEMORY_AND_DISK\n",
    "- MEMORY_ONLY_SER (serialized)\n",
    "- MEMORY_AND_DISK_SER (serialized)\n",
    "- DISK_ONLY\n",
    "\n",
    "The DataFrame operation `.cache()` does not recieve an storage level as under the hood it performs a `.persist()` operation with the storage level `MEMORY_AND_DISK` (Since version 1.3.0) which is usually the most common as it uses memory until is full and then switches to disk storage.\n",
    "\n",
    "```python\n",
    "from pyspark import StorageLevel\n",
    "df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bec850f-9dc1-4252-8e2e-d4d9564b51f0",
   "metadata": {},
   "source": [
    "`NOTE`: Caching is useful with heavy computations which result is not very big (around 50% - 60% of the total executor memory), because if the result is too big then it will start storing it in disk which is noticeably slower to access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4efcdb-0992-45ca-85ce-9f1dd29c978f",
   "metadata": {},
   "source": [
    "## 2. Broadcast Variables and Accumulators\n",
    "\n",
    "### 2.1 Broadcast Variables\n",
    "\n",
    "When working with Spark, you might need to send a read-only variable to all the executors. Broadcast variables allow the programmer to keep a read-only variable stored on each executor rather than shipping a copy of it with tasks. This process helps to reduce the amount of data that needs to be transferred over the network, which can improve performance.\n",
    "\n",
    "<img src=\"images/broadcast_variables.webp\" title=\"Broadcast Variables\" width=\"700px\"/>\n",
    "\n",
    "Here are some scenarios where using broadcast variables in Spark is beneficial:\n",
    "\n",
    "1. **Small DataFrame Joins**: When joining a large DataFrame with a small DataFrame, you can broadcast the smaller DataFrame to all worker nodes. This helps avoid the shuffling of the larger DataFrame and can significantly improve join performance.\n",
    "\n",
    "2. **Frequently Used Lookup Tables**: If tasks often refer to a small lookup table (e.g., mapping of codes to descriptions), it's efficient to broadcast this table so that it's available locally on each worker, rather than fetching it multiple times.\n",
    "\n",
    "3. **Machine Learning Models**: When scoring data using a machine learning model, if the model is small enough, you can broadcast it to all worker nodes. This way, each node can score data locally without needing to fetch the model repeatedly.\n",
    "\n",
    "4. **Configuration or Parameters**: If tasks need to reference certain configuration settings or parameters, broadcasting these can prevent the need to send them with every task.\n",
    "\n",
    "5. **Accumulative Data Structures**: For some algorithms, you might need to refer to a data structure that gets built incrementally (e.g., a prefix tree or histogram). If this structure is small, broadcasting it can improve efficiency.\n",
    "\n",
    "6. **Avoiding Repetitive Reads**: If tasks on each node need to read the same part of a dataset or file, broadcasting the relevant data can help save on I/O operations.\n",
    "\n",
    "7. **Static Data Across Tasks**: In some algorithms or computations, there might be static data or state that doesn't change and is used across tasks. Broadcasting this data ensures it's available locally on each worker.\n",
    "\n",
    "While broadcasting can be highly efficient in these scenarios, there are a few things to keep in mind:\n",
    "\n",
    "- **Size Limitations**: Broadcasting very large variables can be counterproductive. It might use a significant amount of memory on each worker node and also take time to send across the network initially. As a rule of thumb, only broadcast data that is comfortably small relative to the available memory on worker nodes.\n",
    "\n",
    "- **Read-only**: Broadcast variables are meant to be read-only. They should not be modified by tasks.\n",
    "\n",
    "- **Broadcast Cost**: There is an initial cost to broadcast a variable as it needs to be sent to all nodes. It's essential to ensure the benefits of broadcasting (usually in reduced network data transfer in subsequent operations) outweigh this initial cost.\n",
    "\n",
    "#### **Example**\n",
    "\n",
    "Following the previous cache example, let's broadcast the heavy computation to the executors as it is a small DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00292321-5bd1-484f-8e90-0106eacf9a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "big_df.join(broadcast(cached_df), on=\"age\", how=\"left\") \\\n",
    "    .withColumn(\"performance\", \n",
    "        when(col('grade') > col('mean_grade'), 'Over mean') \\\n",
    "        .otherwise(\n",
    "            when(col('grade') == col('mean_grade'), 'On mean') \\\n",
    "            .otherwise('Under mean')\n",
    "        )\n",
    "    ) \\\n",
    "    .select(\"age\", \"name\", \"surname\", \"performance\") \\\n",
    "    .show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d55cad-ee1d-41e2-b2d3-6b6a0352d857",
   "metadata": {},
   "source": [
    "### 2.2 Accumulators\n",
    "\n",
    "Accumulators are variables that can only be \"added\" to. They can be used to implement counters and sums efficiently in parallel. Spark natively supports accumulations of numeric types, and programmers can add support for new types.\n",
    "\n",
    "Here are some scenarios where using accumulators in Spark is beneficial:\n",
    "\n",
    "1. **Global Counters**: Accumulators can be used to maintain global counters across tasks. For example, while processing a large dataset, you might want to keep track of records that meet a certain condition or the total number of errors encountered.\n",
    "\n",
    "2. **Summations**: If you're performing operations that involve summation (e.g., calculating the total sum of a particular field across all records), accumulators can be helpful.\n",
    "\n",
    "3. **Monitoring and Logging**: Accumulators can be used for monitoring purposes. For instance, you might want to track the number of times a particular code path is executed or track the number of missing values for certain fields.\n",
    "\n",
    "4. **Validation and Quality Checks**: While processing data, you might want to perform certain validation checks. Accumulators can be used to count the number of records that fail these validations.\n",
    "\n",
    "5. **Histograms**: They can be employed to build histograms in parallel tasks, where each task updates the histogram based on the portion of data it processes.\n",
    "\n",
    "6. **Set Accumulation**: Though typically accumulators are used for numerical operations like count and sum, they can also be used to accumulate sets. For example, if you want to build a set of unique user agents from web logs.\n",
    "\n",
    "7. **Advanced Algorithms**: In more advanced algorithms, especially those that require global information or intermediate results from different partitions, accumulators can be leveraged to gather this information efficiently.\n",
    "\n",
    "While accumulators are powerful, there are essential caveats and best practices:\n",
    "\n",
    "- **Idempotency**: Accumulators do not guarantee idempotency in case of task failures. If a task is retried by Spark, the accumulator might be updated multiple times for that task. Therefore, they should be used where occasional duplicates are not a concern, or with operations that are both associative and commutative.\n",
    "\n",
    "- **Read-only on Worker Nodes**: Accumulator variables should be updated only on worker nodes (inside Spark transformations). They should not be read on worker nodes. The value of accumulators should only be read on the driver program after all tasks have completed.\n",
    "\n",
    "- **Custom Accumulators**: While Spark provides built-in accumulators for simple types like integer and double, one can also develop custom accumulators for more complex types. However, it's important to ensure that the `add` and `merge` operations are correctly implemented for custom types to ensure accurate and efficient aggregation.\n",
    "\n",
    "In conclusion, accumulators provide an efficient way to gather global information across tasks in Spark.\n",
    "\n",
    "#### **Example**\n",
    "\n",
    "Let's create an accumulator to count how many times any student named \"Santiago\" is processed in a UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1deee41-ccca-46f5-8c30-4b79a4d9fb33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Initialize accumulator\n",
    "santiago_name_accumulator = spark.sparkContext.accumulator(0)\n",
    "\n",
    "# Define UDF function\n",
    "def add_suffix(name):\n",
    "    global santiago_name_accumulator\n",
    "    # Increment if name is Santiago\n",
    "    if name == 'Santiago':\n",
    "        santiago_name_accumulator += 1\n",
    "    return name + \"_UDF\"\n",
    "\n",
    "# Register UDF\n",
    "suffix_udf = udf(add_suffix, StringType())\n",
    "\n",
    "# Apply UDF to DataFrame\n",
    "df.withColumn(\"name_with_suffix\", suffix_udf(df[\"name\"])) \\\n",
    "    .select(\"name\", \"name_with_suffix\") \\\n",
    "    .collect() # Use collect to force the evaluation as Spark has lazy evaluation\n",
    "\n",
    "print(f\"There has been {santiago_name_accumulator.value} students named Santiago processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f5d815-706f-473b-9fe6-b6e479440854",
   "metadata": {},
   "source": [
    "## 3. Partitioning\n",
    "\n",
    "Partitioning in Spark is a way to distribute the data across the distributed storage or computing nodes. Proper partitioning is essential for distributing computation and for optimizing data shuffling and I/O operations. Here's a detailed explanation of when and how to partition data in Spark, along with the types of partitioning strategies:\n",
    "\n",
    "### 3.1 When is it useful to partition data and why?\n",
    "\n",
    "1. **Optimizing Data Locality**: When data and the computation is close to each other, the time to fetch the data is reduced, leading to faster task execution.\n",
    "\n",
    "2. **Optimizing Shuffles**: In operations like joins or groupBy, if the data is properly partitioned, the need to shuffle data across nodes can be minimized, leading to performance improvements.\n",
    "\n",
    "3. **Load Balancing**: Proper partitioning ensures that data is evenly distributed across nodes, preventing situations where some nodes are idle while others are overloaded.\n",
    "\n",
    "4. **Optimizing Joins**: When two DataFrames/RDDs are being joined on a particular column and they are partitioned using the same key, the join operation can be optimized since data is already co-located.\n",
    "\n",
    "5. **Optimizing Data Storage**: When saving data back to distributed storage systems like HDFS or cloud storage, partitioning can optimize storage and subsequent read operations. For instance, data can be partitioned by date, so each day's data is in a separate directory.\n",
    "\n",
    "### 3.2 How to Partition Data in Spark?\n",
    "\n",
    "1. **Repartitioning**: You can use the `repartition()` method on a DataFrame or RDD to increase or decrease the number of partitions. You can optionally specify a column on which to partition.\n",
    "```python\n",
    "df.repartition(100)  # Repartition DataFrame into 100 partitions\n",
    "df.repartition(50, \"date\")  # Repartition based on the 'date' column\n",
    "```\n",
    "\n",
    "2. **Coalesce**: To reduce the number of partitions, `coalesce()` can be more efficient than `repartition()` because it avoids a full shuffle.\n",
    "```python\n",
    "df.coalesce(10)  # Reduce the number of partitions to 10\n",
    "```\n",
    "\n",
    "### 3.3 Types of Partitioning and When to Use Them\n",
    "\n",
    "1. **Hash Partitioning**: Spark distributes data based on the value's hash code. It's useful when you want to ensure a balanced distribution of data but don't necessarily care about which specific keys go to which partition.\n",
    "- **Use Case**: When performing a join operation on a key, using hash partitioning on that key for both DataFrames can optimize the join.\n",
    "\n",
    "2. **Range Partitioning**: The data is partitioned based on a range of values. This ensures that a continuous range of data values resides in a single partition.\n",
    "- **Use Case**: When you have ordered data and operations (like sorting), range partitioning can be beneficial.\n",
    "\n",
    "3. **Custom Partitioning**: Spark allows you to define a custom partitioning strategy by extending `Partitioner` in RDD operations. This way, you can define any specific logic to determine which keys go to which partition.\n",
    "- **Use Case**: When the built-in hash or range partitioning doesn't suit your needs, you might require custom logic.\n",
    "\n",
    "4. **Bucketing**: Specific to DataFrames and used primarily when saving data to storage, bucketing is a form of partitioning where data is divided into a fixed number of \"buckets\" based on the hash of a column's values.\n",
    "- **Use Case**: Useful when you have a large dataset that you'll be querying frequently with filters on a specific column. For instance, saving data in buckets based on user ID for a large user dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb28795-42cc-40ee-b00c-de037260b54f",
   "metadata": {},
   "source": [
    "### 4.4 Example\n",
    "In the `datasets/` folder we have a [Kaggle dataset](https://www.kaggle.com/datasets/mvieira101/global-cost-of-living?select=cost-of-living_v2.csv) named `cost-of-living.csv` which contains the following information:\n",
    "\n",
    "- `city`: Name of the city\n",
    "- `country`: Name of the country\n",
    "- `x1`: Meal, Inexpensive Restaurant (USD)\n",
    "- `x2`: Meal for 2 People, Mid-range Restaurant, Three-course (USD)\n",
    "- `x3`: McMeal at McDonalds (or Equivalent Combo Meal) (USD)\n",
    "- `x4`: Domestic Beer (0.5 liter draught, in restaurants) (USD)\n",
    "- `x5`: Imported Beer (0.33 liter bottle, in restaurants) (USD)\n",
    "- `x6`: Cappuccino (regular, in restaurants) (USD)\n",
    "- `x7`: Coke/Pepsi (0.33 liter bottle, in restaurants) (USD)\n",
    "- `x8`: Water (0.33 liter bottle, in restaurants) (USD)\n",
    "- `x9`: Milk (regular), (1 liter) (USD)\n",
    "- `x10`: Loaf of Fresh White Bread (500g) (USD)\n",
    "- `x11`: Rice (white), (1kg) (USD)\n",
    "- `x12`: Eggs (regular) (12) (USD)\n",
    "- `x13`: Local Cheese (1kg) (USD)\n",
    "- `x14`: Chicken Fillets (1kg) (USD)\n",
    "- `x15`: Beef Round (1kg) (or Equivalent Back Leg Red Meat) (USD)\n",
    "- `x16`: Apples (1kg) (USD)\n",
    "- `x17`: Banana (1kg) (USD)\n",
    "- `x18`: Oranges (1kg) (USD)\n",
    "- `x19`: Tomato (1kg) (USD)\n",
    "- `x20`: Potato (1kg) (USD)\n",
    "- `x21`: Onion (1kg) (USD)\n",
    "- `x22`: Lettuce (1 head) (USD)\n",
    "- `x23`: Water (1.5 liter bottle, at the market) (USD)\n",
    "- `x24`: Bottle of Wine (Mid-Range, at the market) (USD)\n",
    "- `x25`: Domestic Beer (0.5 liter bottle, at the market) (USD)\n",
    "- `x26`: Imported Beer (0.33 liter bottle, at the market) (USD)\n",
    "- `x27`: Cigarettes 20 Pack (Marlboro) (USD)\n",
    "- `x28`: One-way Ticket (Local Transport) (USD)\n",
    "- `x29`: Monthly Pass (Regular Price) (USD)\n",
    "- `x30`: Taxi Start (Normal Tariff) (USD)\n",
    "- `x31`: Taxi 1km (Normal Tariff) (USD)\n",
    "- `x32`: Taxi 1hour Waiting (Normal Tariff) (USD)\n",
    "- `x33`: Gasoline (1 liter) (USD)\n",
    "- `x34`: Volkswagen Golf 1.4 90 KW Trendline (Or Equivalent New Car) (USD)\n",
    "- `x35`: Toyota Corolla Sedan 1.6l 97kW Comfort (Or Equivalent New Car) (USD)\n",
    "- `x36`: Basic (Electricity, Heating, Cooling, Water, Garbage) for 85m2 Apartment (USD)\n",
    "- `x37`: 1 min. of Prepaid Mobile Tariff Local (No Discounts or Plans) (USD)\n",
    "- `x38`: Internet (60 Mbps or More, Unlimited Data, Cable/ADSL) (USD)\n",
    "- `x39`: Fitness Club, Monthly Fee for 1 Adult (USD)\n",
    "- `x40`: Tennis Court Rent (1 Hour on Weekend) (USD)\n",
    "- `x41`: Cinema, International Release, 1 Seat (USD)\n",
    "- `x42`: Preschool (or Kindergarten), Full Day, Private, Monthly for 1 Child (USD)\n",
    "- `x43`: International Primary School, Yearly for 1 Child (USD)\n",
    "- `x44`: 1 Pair of Jeans (Levis 501 Or Similar) (USD)\n",
    "- `x45`: 1 Summer Dress in a Chain Store (Zara, H&M, ‚Ä¶) (USD)\n",
    "- `x46`: 1 Pair of Nike Running Shoes (Mid-Range) (USD)\n",
    "- `x47`: 1 Pair of Men Leather Business Shoes (USD)\n",
    "- `x48`: Apartment (1 bedroom) in City Centre (USD)\n",
    "- `x49`: Apartment (1 bedroom) Outside of Centre (USD)\n",
    "- `x50`: Apartment (3 bedrooms) in City Centre (USD)\n",
    "- `x51`: Apartment (3 bedrooms) Outside of Centre (USD)\n",
    "- `x52`: Price per Square Meter to Buy Apartment in City Centre (USD)\n",
    "- `x53`: Price per Square Meter to Buy Apartment Outside of Centre (USD)\n",
    "- `x54`: Average Monthly Net Salary (After Tax) (USD)\n",
    "- `x55`: Mortgage Interest Rate in Percentages (%), Yearly, for 20 Years Fixed-Rate\n",
    "- `data_quality`: 0 if Numbeo considers that more contributors are needed to increase data quality, else 1\n",
    "\n",
    "We want to know the countries with the highest average cost for a Capuccino (`x6`), you know, we are Spark developers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15245b87-277d-4e96-8eac-ecaa55430dcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\",True).csv(\"datasets/cost-of-living.csv\").select(\"city\", \"country\", \"x6\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ab443c-ea54-4888-a211-d96b6ae711e6",
   "metadata": {},
   "source": [
    "As we can see by printing the schema the column `x6` is a `string`, so we need to cast it into a `float`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccc7e83-9d04-4be7-8c36-41dcde852fc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumn(\"capuccino_price\", col(\"x6\").cast(\"float\")).drop(\"x6\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ca9412-40e1-421d-b3f7-432f0f4e3815",
   "metadata": {},
   "source": [
    "Now, the column `capuccino_price` is a `float` but is nullable which means some values of the price can be null (NaN as it is a float), in this case we will check if there are any null values and remove them as we dont want to compute the mean with NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc271d51-fae8-4f61-8f77-898d70649c71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan\n",
    "\n",
    "print(\"#### Unfiltered DataFrame ####\")\n",
    "df.filter(isnan(\"capuccino_price\")).show(5)\n",
    "df = df.filter(~isnan(\"capuccino_price\"))\n",
    "print(\"#### Filtered DataFrame ####\")\n",
    "df.filter(isnan(\"capuccino_price\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acd4eb4-02d1-4272-81bb-b4f32474fe51",
   "metadata": {},
   "source": [
    "Now we can start computing the countries with the highest average cost for a Capuccino. But in order to appreciate a time difference we will create a bigger DataFrame like before by unioning it with itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91685cd6-183b-4b89-8ecf-1b9b9500cdc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "big_df = df\n",
    "\n",
    "for _ in range(99):\n",
    "    big_df = big_df.union(df)\n",
    "\n",
    "print(f\"The new DataFrame has {big_df.count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a96564-cdfc-48af-ba6c-59002489d54a",
   "metadata": {},
   "source": [
    "**Without Proper Partitioning:**\n",
    "\n",
    "If we don't consider partitioning, the default partitioning strategy might distribute the data across nodes without any specific ordering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7ebc00-1af5-4f63-85ff-738ceba86ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.sql.functions import mean, format_number, desc\n",
    "\n",
    "big_df.groupBy(\"country\") \\\n",
    "    .agg(\n",
    "        format_number(\n",
    "            mean(\"capuccino_price\"), 2\n",
    "        ).alias(\"avg_capuccino_price\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"avg_capuccino_price\")) \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc2d8d0-3ef3-4577-8944-e474491cb0da",
   "metadata": {},
   "source": [
    "In this case, since data isn't partitioned by `country`, there's a high chance that data shuffling will occur when grouping by `country`. This can result in higher network overhead and slower performance.\n",
    "\n",
    "`Side Note`: Wow! Didn't see that one coming, but seems legit according to the page [Coffeestics](https://coffeestics.com/countries/turkmenistan) ü§∑‚Äç‚ôÇÔ∏è."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098badd5-af9f-4cb9-b6d0-fc3906076bef",
   "metadata": {},
   "source": [
    "**With Proper Partitioning:**\n",
    "\n",
    "By repartitioning our data by the `country` column, we can ensure that all data for a given `country` will reside on the same partition (and thus the same node):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74c9515-7c56-4309-a5a1-5710d2f050ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_df = big_df.repartition(\"country\")\n",
    "partitioned_df.write.mode(\"overwrite\").parquet(\"datasets/partitioned_cost_of_living\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437fe664-d925-4205-a54e-2cb19fc7bb6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"datasets/partitioned_cost_of_living\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f118cc22-b407-4c83-b94e-6c56696f2b85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df.groupBy(\"country\") \\\n",
    "    .agg(\n",
    "        format_number(\n",
    "            mean(\"capuccino_price\"), 2\n",
    "        ).alias(\"avg_capuccino_price\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"avg_capuccino_price\")) \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35095c9b-174f-41bf-9611-2905dbe7d1f4",
   "metadata": {},
   "source": [
    "Now, when we perform the `groupBy` operation, PySpark will not need to shuffle data over the network, because all the data for a particular country is already co-located on the same node. This reduces network overhead and speeds up the operation.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "This example illustrates the importance of good partitioning in PySpark. By being aware of how our data is distributed and how it will be accessed, we can leverage partitioning to optimize performance. Proper partitioning reduces data shuffling and ensures that related data is co-located, leading to more efficient computations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
