{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13f93ee7-8060-4ceb-9f18-e02e6d80d158",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064c9cb9-27ef-4170-b848-0ed78f205197",
   "metadata": {},
   "source": [
    "## 1. Introduction to DataFrames and Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17babb4b-e1e1-44f7-93c1-c0d6d32358c7",
   "metadata": {},
   "source": [
    "### Difference between RDDs, DataFrames and DataSets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa40c0da-46e4-499a-ad27-763cf678eb02",
   "metadata": {},
   "source": [
    "**RDDs (2011)**:\n",
    "- Distribute collection of Java Virtual Machine (JVM) objects\n",
    "- Functional Operators (map, reduce, filter, etc)\n",
    "\n",
    "**DataFrame (2013)**:\n",
    "- Distributed collection of Row objects\n",
    "- Expression based operations and User Defined Functions (UDFs)\n",
    "- Logical plans and optimizer\n",
    "- Fast/efficient internal representations\n",
    "\n",
    "**DataSet (2015)**:\n",
    "- Internally Rows, externally JVM objects\n",
    "- Almost the \"Best of both worlds\": type safe + fast\n",
    "- Slower than DataFrame, not as good for interactive analysis, especially Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa75e74-eca4-4b05-a43d-e13af7b83f2b",
   "metadata": {},
   "source": [
    "## 2. DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dba8a0-db60-44cd-8204-d65d0c5ee4f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"images/dataframe_concept.png\" title=\"DataFrame Concept\" width=\"700px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3149a7bd-1a73-41cb-bbeb-e5e596b0ac52",
   "metadata": {},
   "source": [
    "### Basic concepts\n",
    "- Distributed collection of Row objects. These objects contain the schema within the data.\n",
    "- Data is organized into columns like a relational database.\n",
    "- The main features of Dataframes:\n",
    "- **Catalyst**: powers the Dataframe and SQL APIs.\n",
    "    1. Analyzing a logical plan to resolve references\n",
    "    2. Logical plan optimization\n",
    "    3. Physical planning\n",
    "    4. Code generation to compile parts of the query to Java bytecode.\n",
    "- **Tungsten**: provides a physical execution backend which explicitly manages memory and dynamically generates bytecode for expression evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1602556d-7ceb-4be7-84f6-356fa02afc53",
   "metadata": {},
   "source": [
    "### Creating DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c776470e-34f8-4121-a3ef-2be9623b5f57",
   "metadata": {},
   "source": [
    "DataFrames can be created from many different sources such as existing RDDs, structured and unstructured data files (CSV, JSON, Parquet), databases using JDBC, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2632fb91-78c9-4d91-ba84-bbd8dbcfd56a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import Row, IntegerType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\n",
    "\n",
    "# Creating DataFrame from an existing RDD\n",
    "rdd = spark.sparkContext.parallelize([0, 1, 2, 3, 4, 5])\n",
    "df = rdd.map(lambda x: Row(x)).toDF()\n",
    "print(\"DataFrame from RDD.toDF() function\")\n",
    "df.show()\n",
    "schema = IntegerType()\n",
    "df = spark.createDataFrame(rdd, schema)\n",
    "print(\"DataFrame from RDD with saprk.createDataFrame()\")\n",
    "df.show()\n",
    "\n",
    "# Creating a DataFrame from a Python collection\n",
    "cars = [\n",
    "    {\"brand\": \"Toyota\", \"name\": \"Camry\", \"price\": 24000},\n",
    "    {\"brand\": \"Honda\", \"name\": \"Civic\", \"price\": 22000},\n",
    "    {\"brand\": \"Ford\", \"name\": \"Mustang\", \"price\": 27000},\n",
    "    {\"brand\": \"Tesla\", \"name\": \"Model 3\", \"price\": 35000},\n",
    "    {\"brand\": \"Chevrolet\", \"name\": \"Malibu\", \"price\": 23000}\n",
    "]\n",
    "df = spark.createDataFrame(cars)\n",
    "print(\"DataFrame from a Python collection\")\n",
    "df.show()\n",
    "\n",
    "# Creating a DataFrame from a JSON file\n",
    "df = spark.read.json('datasets/students.json')\n",
    "print(\"DataFrame from a JSON file\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be53ba4-5878-4818-93fd-ad3807a3314c",
   "metadata": {},
   "source": [
    "## 3. Running SQL Queries using Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc57b4d2-3a3e-44bc-a9b7-5fba3a11a7cd",
   "metadata": {},
   "source": [
    "In Spark you can work with DataFrames using the built-in functions or the Spark SQL API that provides a syntax similar to standard SQL but includes many extra ones that can be found [here](https://spark.apache.org/docs/latest/api/sql/index.html) (I recommend you to add this page to your bookmarks!). \n",
    "\n",
    "`NOTE`: Depending on the Spark version you are using this functions may change, you can find all the available Spark Docs for the different versions [here](https://spark.apache.org/docs/). To check you Spark version print your Spark Session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33320345-9215-4d27-8d7a-70e8d8569528",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca7eb6f-fd26-4199-b9ab-cf1a5972e2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, lets check the schema of the df we will work with\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e25a71-ec0d-4c57-aa3b-e0e7211ac0a9",
   "metadata": {},
   "source": [
    "### Reading the top 10 lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa26b472-6e6c-4a0b-b9a9-a8ef30258e97",
   "metadata": {},
   "source": [
    "#### DataFrame mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177c2f98-6747-4353-879c-53d96d86aa23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ee52600-6114-4d8e-97f4-cab1a8124436",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### SQL mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e724aafb-51af-4c8c-9912-f8fdbc98d554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d654870-f1a8-4c27-af43-1de5965c877e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61ca4b23-7867-48d0-9783-35f075193a53",
   "metadata": {},
   "source": [
    "### Understanding the Pyshical and Logical plans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8b3137-0e1a-4ca5-9b58-1efca6405302",
   "metadata": {},
   "source": [
    "As previously stated, the **Catalyst** is in charge of creating an optimizing the plan that will be converted into a DAG and sent to the executors. The following diagram shows the steps that the Catalyst performs to produce an execution plan\n",
    "\n",
    "<img src=\"images/catalyst_steps.webp\" title=\"Catalyst Steps Diagram\" width=\"700px\"/>\n",
    "\n",
    "In order to see the plan generated by the **Catalyst** you can call the function `explain()` which, by default, will display the Physical Plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9080e7-f7ca-4c5d-ac29-52544716d5ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c94db648-908f-4942-82db-3337a85eb32e",
   "metadata": {},
   "source": [
    "#### Getting various plans\n",
    "Before Apache Spark 3.0, there was only two modes available to format explain output.\n",
    "\n",
    "- `explain(extended=False)` which displayed only the physical plan\n",
    "- `explain(extended=True)` which displayed all the plans (logical and physical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be28fc24-866e-4205-90e9-e8b9d85e2325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f189dd2e-ad27-45b6-967f-0d94ff4ef0eb",
   "metadata": {},
   "source": [
    "Starting from Apache Spark 3.0, you have a new parameter “mode” that produce expected format for the plan:\n",
    "\n",
    "- `explain(mode=\"simple\")` which will display the physical plan\n",
    "- `explain(mode=\"extended\")` which will display physical and logical plans (like \"extended\" option)\n",
    "- `explain(mode=\"codegen\")` which will display the java code planned to be executed\n",
    "- `explain(mode=\"cost\")` which will display the optimized logical plan and related statistics (if they exist)\n",
    "- `explain(mode=\"formatted\")` which will display a splitted output composed by a nice physical plan outline, and a section with each node details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487acccb-d035-414b-a687-73e37a5fa8a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1aa4a18e-a31e-431a-82a3-a3f330e61941",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Top 5 students with higher grades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a197fab-f130-492c-b6dc-afdff282221f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### DataFrame mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b8d32d-dfc3-428f-a292-a1d86116782c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7decc916-e4d7-4281-a8ed-ff9e5f4dc50f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### SQL mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc009d5-4936-4fe7-953d-7865357e95f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9687683b-9307-44e1-b249-e85ccece6611",
   "metadata": {},
   "source": [
    "### Max and Mean grade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16416cee-2fe0-42f6-946f-594822dfc9cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### DataFrame mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e2143c-5c3e-49ae-9da5-3ecb671fdc30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a2714d0-12ae-4d8c-a309-6d46a25d84e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### SQL mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1764dc5a-4294-41a4-8eed-dfa20f74ffc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d91a387-ebc6-44f8-bfd4-e64c0c09352e",
   "metadata": {},
   "source": [
    "### Top 5 students with highest mean grade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2810febf-15b6-4b7b-93f5-d2706383f8d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### DataFrame mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5d041a-f608-4cf0-9b3c-67014b64c0b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21f62933-8d60-4889-883f-efb6747a5962",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### SQL mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddcfad9-57c0-437e-8436-146031d22d20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c68bf89a-186a-4c52-8314-f1883c7ab36b",
   "metadata": {},
   "source": [
    "### Average grade per student age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d0443a-6d13-4e80-a9f1-39709223429c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### DataFrame mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14170333-adaa-480f-8113-75ee80b95123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7758d3cf-8e0c-46e9-9cf9-724cb47e3397",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### SQL mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3024835e-d24d-4897-9f85-5a35a9a1942b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4ca37c1-9301-41e0-9ba5-60aaccd42624",
   "metadata": {},
   "source": [
    "### Add column `excelent` to indicate that a student has a grade over 9.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab9c56a-9564-49d6-8607-1e49da312a70",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### DataFrame mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b350e3-fa38-4a0c-847d-72b72418c5d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f31b22fe-a23c-4da3-acf3-2eb9cce7a2ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### SQL mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dfa5b6-0d51-4564-87d9-78bcdfd1fd00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5edddc7-3990-4856-81bb-6f7baea617d1",
   "metadata": {},
   "source": [
    "## 4. Spark SQL Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aeb6b6-d5d7-42b0-97cb-73c39cb4b496",
   "metadata": {},
   "source": [
    "Spark DataFrame supports all basic SQL Join Types like `INNER`, `LEFT OUTER`, `RIGHT OUTER`, `LEFT ANTI`, `LEFT SEMI`, `CROSS`, `FULL OUTER`, etc. Spark SQL Joins are wide transformations that result in data shuffling over the network hence they have huge performance issues when not designed with care.\n",
    "\n",
    "On the other hand Spark SQL Joins comes with more optimization by default (thanks to DataFrames & Dataset) however still there would be some performance issues to consider while using them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1388506-e203-49cd-ab5a-81fd88dc2d9d",
   "metadata": {},
   "source": [
    "### Most common types of Joins\n",
    "\n",
    "<img src=\"images/joins.jpg\" title=\"Types of Joins\" width=\"700px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c79a759-6e25-469a-b5f9-951959e7601d",
   "metadata": {},
   "source": [
    "### How to perform a Join in Spark (cheat sheet)\n",
    "\n",
    "1. Referencing columns with different names in each df:\n",
    "\n",
    "```python\n",
    "dfA.join(dfB, dfA[\"idA\"] == dfB[\"idB\"], \"type-of-join\")\n",
    "\n",
    "```\n",
    "\n",
    "2. Key column has the same alias\n",
    "\n",
    "```python\n",
    "dfA.join(dfB, on=\"id\", how=\"type-of-join\")\n",
    "\n",
    "```\n",
    "\n",
    "3. More than one key column\n",
    "\n",
    "```python\n",
    "dfA.join(dfB, [\"id\", \"code\"], \"type-of-join\")\n",
    "\n",
    "dfA.join(dfB, (dfA[\"id\"] == dfB[\"id\"]) & (dfA[\"code\"] == dfB[\"code\"]), \"type-of-join\")\n",
    "\n",
    "```\n",
    "\n",
    "4. Maintaining both columns (be careful and be sure of referencing the origin of the column when using it after this)\n",
    "\n",
    "```python\n",
    "dfC = dfA.join(dfB, dfA[\"id\"] == dfB[\"id\"], \"type-of-join\")\n",
    "\n",
    "dfC.select(dfA[\"id\"]).show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd9c5ad-43ee-492a-9eb8-dee1f6ed56ee",
   "metadata": {},
   "source": [
    "### Read the example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daaf593-e0c0-4332-a47e-41514847ac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emp = spark.read.json(\"datasets/employees.json\")\n",
    "df_emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adacfc8-e829-4052-9770-41ec7fd9dfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dept = spark.read.json(\"datasets/departments.json\")\n",
    "df_dept.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e03c03-683f-478b-84a7-c7d0179f70d0",
   "metadata": {},
   "source": [
    "### Inner Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97e7753-a993-411f-bfa8-c896dec9d28e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "154f72a3-6a63-4ec2-a4bb-81a8989f6a07",
   "metadata": {},
   "source": [
    "- The employee that belongs to inexistent department with id 50 is dropped\n",
    "- The Sales department that has no employees asociated is dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51172a5-7a61-4f0c-bcf3-889044cceac8",
   "metadata": {},
   "source": [
    "### Full Outer Join\n",
    "\n",
    "Outer a.k.a full, fullouter join returns all rows from both Spark DataFrame, where join expression doesn’t match it returns null on respective record columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129a4cfe-8690-4b0c-8e80-9c71061c63cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf5848c1-d807-475b-a96e-d37d460031a2",
   "metadata": {},
   "source": [
    "- There is no department with id 50 hence the values for the department are null\n",
    "- There are no employees in the Sales department hence the employee values are null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f5335d-b732-4ce7-9e64-e3a825ba6cc5",
   "metadata": {},
   "source": [
    "### Left Join\n",
    "\n",
    "Left a.k.a Left Outer join returns all rows from the left DataFrame/Dataset regardless of match found on the right dataset when join expression doesn’t match, it assigns null for that record and drops records from right where match not found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7215ac3-e9a7-483e-a32c-fc10fb51c2a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "327fe8a8-84ca-4ccc-9b7c-59eef5bfb81c",
   "metadata": {},
   "source": [
    "- There is no department with id 50 hence the values for the department are null\n",
    "- There are no employees in the Sales department hence no row is shown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ffa2d4-963d-4701-87d9-f1700bff149e",
   "metadata": {},
   "source": [
    "### Right Join\n",
    "\n",
    "Right a.k.a Right Outer join is opposite to left join, here it returns all rows from the right DataFrame/Dataset regardless of match found on the left dataset, when join expression doesn’t match, it assigns null for that record and drops records from left where match not found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa185ac6-3135-4b1c-88dd-483243a3f548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e89fcaf6-0350-40ff-8a62-92514160ccbc",
   "metadata": {},
   "source": [
    "- There is no department with id 50 hence no row is shown\n",
    "- There are no employees in the Sales department hence the values of the employee columns are null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f215108-ed56-446f-816e-dc6c8c1962c4",
   "metadata": {},
   "source": [
    "### CrossJoin\n",
    "\n",
    "This join combines each row of the first table with each row of the second table. For example, we have `m` rows in one table and `n` rows in another, this gives us `m * n` rows in the resulting table. \n",
    "\n",
    "`Note`: A table of 1000 customers combined with a table of 1000 products would produce 1,000,000 records! Try to avoid this with large tables in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1a776b-66e7-4cd2-91c3-330e524be475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b87ba738-e14e-4720-87d2-856aac5e86c7",
   "metadata": {},
   "source": [
    "## 5. User Defined Functions (UDFs)\n",
    "User Defined Functions (UDFs) in Spark allow users to define their own transformations using Python or other programming languages, and then apply those functions on a Spark DataFrame. This can be very powerful when you need to make specific transformations to your data that aren't easily achieved using Spark's built-in functions.\n",
    "\n",
    "When a UDF is defined, under the hood, Spark serializes the function using Py4J, transfers it over to the executor nodes, and deserializes it. This allows the UDF to be executed on rows of the DataFrame in parallel. However, it's worth noting that because UDFs involve serialization and data transfer between Python and JVM, they can be considerably slower than using native Spark functions.\n",
    "\n",
    "Here's how you can define and use a UDF in Spark with Python (PySpark):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3e9331-bbaf-4b02-a8e1-cd4dfa1c3956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def add_suffix(name):\n",
    "    return name + \"_UDF\"\n",
    "\n",
    "suffix_udf = udf(add_suffix, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ec4ca0-0a35-4b8f-8b26-66e275fa5113",
   "metadata": {},
   "source": [
    "#### DataFrame Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4e8935-46b0-4c6e-ada3-0ddb0f257a85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee880086-337e-474e-9874-634c61600498",
   "metadata": {},
   "source": [
    "#### SQL Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba634cfd-db1f-45fc-83b1-55910742e9c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
