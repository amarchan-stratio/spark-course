{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13f93ee7-8060-4ceb-9f18-e02e6d80d158",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064c9cb9-27ef-4170-b848-0ed78f205197",
   "metadata": {},
   "source": [
    "## 1. Introduction to DataFrames and Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17babb4b-e1e1-44f7-93c1-c0d6d32358c7",
   "metadata": {},
   "source": [
    "### Difference between RDDs, DataFrames and DataSets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa40c0da-46e4-499a-ad27-763cf678eb02",
   "metadata": {},
   "source": [
    "**RDDs (2011)**:\n",
    "- Distribute collection of Java Virtual Machine (JVM) objects\n",
    "- Functional Operators (map, reduce, filter, etc)\n",
    "\n",
    "**DataFrame (2013)**:\n",
    "- Distribute collection of Row objects\n",
    "- Expression based operations and User Defined Functions (UDFs)\n",
    "- Logical plans and optimizer\n",
    "- Fast/efficient internal representations\n",
    "\n",
    "**DataSet (2015)**:\n",
    "- Internally Rows, externally JVM objects\n",
    "- Almost the \"Best of both worlds\": type safe + fast\n",
    "- Slower than DataFrame, not as good for interactive analysis, especially Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa75e74-eca4-4b05-a43d-e13af7b83f2b",
   "metadata": {},
   "source": [
    "## 2. DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dba8a0-db60-44cd-8204-d65d0c5ee4f2",
   "metadata": {},
   "source": [
    "![DataFrame Concept](../images/dataframe_concept.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3149a7bd-1a73-41cb-bbeb-e5e596b0ac52",
   "metadata": {},
   "source": [
    "### Basic concepts\n",
    "- Distributed collection of Row objects. These objects contain the schema within the data.\n",
    "- Data is organized into columns like a relational database.\n",
    "- The main features of Dataframes:\n",
    "- **Catalyst**: powers the Dataframe and SQL APIs.\n",
    "    1. Analyzing a logical plan to resolve references\n",
    "    2. Logical plan optimization\n",
    "    3. Physical planning\n",
    "    4. Code generation to compile parts of the query to Java bytecode.\n",
    "- **Tungsten**: provides a physical execution backend which explicitly manages memory and dynamically generates bytecode for expression evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1602556d-7ceb-4be7-84f6-356fa02afc53",
   "metadata": {},
   "source": [
    "### Creating DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c776470e-34f8-4121-a3ef-2be9623b5f57",
   "metadata": {},
   "source": [
    "DataFrames can be created from many different sources such as existing RDDs, structured and unstructured data files (CSV, JSON, Parquet), databases using JDBC, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efab3dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!echo $JAVA_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2632fb91-78c9-4d91-ba84-bbd8dbcfd56a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame from RDD.toDF() function\n",
      "+---+\n",
      "| _1|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "+---+\n",
      "\n",
      "DataFrame from RDD with saprk.createDataFrame()\n",
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    0|\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "|    4|\n",
      "|    5|\n",
      "+-----+\n",
      "\n",
      "DataFrame from a Python collection\n",
      "+---------+-------+-----+\n",
      "|    brand|   name|price|\n",
      "+---------+-------+-----+\n",
      "|   Toyota|  Camry|24000|\n",
      "|    Honda|  Civic|22000|\n",
      "|     Ford|Mustang|27000|\n",
      "|    Tesla|Model 3|35000|\n",
      "|Chevrolet| Malibu|23000|\n",
      "+---------+-------+-----+\n",
      "\n",
      "DataFrame from a JSON file\n",
      "+--------------------+---+--------------------+-----+--------+-----------------+\n",
      "|                 _id|age|               email|grade|    name|          surname|\n",
      "+--------------------+---+--------------------+-----+--------+-----------------+\n",
      "|33a624e7-e6f1-40b...| 23|Valeria.Sebastian...| 7.56| Valeria| Sebastian Garcia|\n",
      "|2cd47675-43f3-415...| 23|Sanchez.Abascal@g...| 8.16|    Emma|  Sanchez Abascal|\n",
      "|594ea4e7-75e3-456...| 20|Sarabia.Lopez@gma...| 8.22| Agustin|    Sarabia Lopez|\n",
      "|3b521244-d2d4-40b...| 25|MartinaySebastian...| 7.67| Martina|Corominas Sarabia|\n",
      "|e6f52130-362f-4a5...| 19|DavidyValeria@gma...| 7.45|   David|   Miranda Grande|\n",
      "|cee04454-f6ea-48b...| 20|Lopez.Bernal@outl...| 7.35|    Laia|     Lopez Bernal|\n",
      "|6e5b75cd-0d5f-41f...| 22|MarcosySantiago@h...|  6.8|  Marcos|     Garcia Aznar|\n",
      "|47435195-80b1-473...| 18|Judith.Garcia@gma...|  9.1|  Judith|      Garcia Cruz|\n",
      "|fbdf66dc-49da-467...| 21| IkerySara@gmail.com| 7.77|    Iker|    Seco Coronado|\n",
      "|0df69140-84ac-47d...| 22|Lopez.Sastre@hotm...|  7.9|   Pablo|     Lopez Sastre|\n",
      "|dbd1026a-b18f-450...| 19|Marcos1997@gmail.com|  7.8|  Marcos|  Sarabia Gisbert|\n",
      "|ee2568cb-93f8-44c...| 24|  Oriol1992@yahoo.es| 7.39|   Oriol|   Blanes Miranda|\n",
      "|2f5239a1-05c0-454...| 18|Marcos.Cuenca@yah...|  9.8|  Marcos|     Cuenca Lopez|\n",
      "|40191301-f2b6-4aa...| 23|Sandra.Sastre@gma...|  8.3|  Sandra| Sastre Corominas|\n",
      "|eac69540-3a1d-4a3...| 22|LuciayTomas@outlo...| 8.69|   Lucia|      Agudo Lopez|\n",
      "|1f58ebe4-9fc5-429...| 29|MarcosyMario@gmai...|  8.8|  Marcos|   Garcia Sanchez|\n",
      "|54d4a8d3-4cc1-4dc...| 24|EmmayJose@hotmail.es| 9.25|    Emma|     Cruz Antunez|\n",
      "|33f54c17-2a1b-421...| 21|Santander.Pascual...| 5.79|   Pedro|Santander Pascual|\n",
      "|971deb27-156d-4cd...| 21|Aznar.Pascual@idx...|  8.6|Santiago|    Aznar Pascual|\n",
      "|cb1244e2-63cf-4a7...| 20|Juan.Parada@gmail...|  8.3|    Juan|   Parada Pascual|\n",
      "+--------------------+---+--------------------+-----+--------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import Row, IntegerType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\n",
    "\n",
    "# Creating DataFrame from an existing RDD\n",
    "rdd = spark.sparkContext.parallelize([0, 1, 2, 3, 4, 5])\n",
    "df = rdd.map(lambda x: Row(x)).toDF()\n",
    "print(\"DataFrame from RDD.toDF() function\")\n",
    "df.show()\n",
    "schema = IntegerType()\n",
    "df = spark.createDataFrame(rdd, schema)\n",
    "print(\"DataFrame from RDD with saprk.createDataFrame()\")\n",
    "df.show()\n",
    "\n",
    "# Creating a DataFrame from a Python collection\n",
    "cars = [\n",
    "    {\"brand\": \"Toyota\", \"name\": \"Camry\", \"price\": 24000},\n",
    "    {\"brand\": \"Honda\", \"name\": \"Civic\", \"price\": 22000},\n",
    "    {\"brand\": \"Ford\", \"name\": \"Mustang\", \"price\": 27000},\n",
    "    {\"brand\": \"Tesla\", \"name\": \"Model 3\", \"price\": 35000},\n",
    "    {\"brand\": \"Chevrolet\", \"name\": \"Malibu\", \"price\": 23000}\n",
    "]\n",
    "df = spark.createDataFrame(cars)\n",
    "print(\"DataFrame from a Python collection\")\n",
    "df.show()\n",
    "\n",
    "# Creating a DataFrame from a JSON file\n",
    "df = spark.read.json('../datasets/students.json')\n",
    "print(\"DataFrame from a JSON file\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be53ba4-5878-4818-93fd-ad3807a3314c",
   "metadata": {},
   "source": [
    "## 3. Running SQL Queries using Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc57b4d2-3a3e-44bc-a9b7-5fba3a11a7cd",
   "metadata": {},
   "source": [
    "In Spark you can work with DataFrames using the built-in functions or the Spark SQL API that provides a syntax similar to standard SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ca7eb6f-fd26-4199-b9ab-cf1a5972e2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- grade: double (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- surname: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First, lets check the schema of the df we will work with\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e25a71-ec0d-4c57-aa3b-e0e7211ac0a9",
   "metadata": {},
   "source": [
    "### Reading the top 10 lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa26b472-6e6c-4a0b-b9a9-a8ef30258e97",
   "metadata": {},
   "source": [
    "#### DataFrame mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "177c2f98-6747-4353-879c-53d96d86aa23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+--------------------+-----+-------+-----------------+\n",
      "|                 _id|age|               email|grade|   name|          surname|\n",
      "+--------------------+---+--------------------+-----+-------+-----------------+\n",
      "|33a624e7-e6f1-40b...| 23|Valeria.Sebastian...| 7.56|Valeria| Sebastian Garcia|\n",
      "|2cd47675-43f3-415...| 23|Sanchez.Abascal@g...| 8.16|   Emma|  Sanchez Abascal|\n",
      "|594ea4e7-75e3-456...| 20|Sarabia.Lopez@gma...| 8.22|Agustin|    Sarabia Lopez|\n",
      "|3b521244-d2d4-40b...| 25|MartinaySebastian...| 7.67|Martina|Corominas Sarabia|\n",
      "|e6f52130-362f-4a5...| 19|DavidyValeria@gma...| 7.45|  David|   Miranda Grande|\n",
      "|cee04454-f6ea-48b...| 20|Lopez.Bernal@outl...| 7.35|   Laia|     Lopez Bernal|\n",
      "|6e5b75cd-0d5f-41f...| 22|MarcosySantiago@h...|  6.8| Marcos|     Garcia Aznar|\n",
      "|47435195-80b1-473...| 18|Judith.Garcia@gma...|  9.1| Judith|      Garcia Cruz|\n",
      "|fbdf66dc-49da-467...| 21| IkerySara@gmail.com| 7.77|   Iker|    Seco Coronado|\n",
      "|0df69140-84ac-47d...| 22|Lopez.Sastre@hotm...|  7.9|  Pablo|     Lopez Sastre|\n",
      "+--------------------+---+--------------------+-----+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee52600-6114-4d8e-97f4-cab1a8124436",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### SQL mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e724aafb-51af-4c8c-9912-f8fdbc98d554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|         | students|       true|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This line only needs to be executed once to create a temporal view in the Spark Session\n",
    "df.createOrReplaceTempView(\"students\")\n",
    "# You can check the tables you have defined in a Spark Session with the following line\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d654870-f1a8-4c27-af43-1de5965c877e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+--------------------+-----+-------+-----------------+\n",
      "|                 _id|age|               email|grade|   name|          surname|\n",
      "+--------------------+---+--------------------+-----+-------+-----------------+\n",
      "|33a624e7-e6f1-40b...| 23|Valeria.Sebastian...| 7.56|Valeria| Sebastian Garcia|\n",
      "|2cd47675-43f3-415...| 23|Sanchez.Abascal@g...| 8.16|   Emma|  Sanchez Abascal|\n",
      "|594ea4e7-75e3-456...| 20|Sarabia.Lopez@gma...| 8.22|Agustin|    Sarabia Lopez|\n",
      "|3b521244-d2d4-40b...| 25|MartinaySebastian...| 7.67|Martina|Corominas Sarabia|\n",
      "|e6f52130-362f-4a5...| 19|DavidyValeria@gma...| 7.45|  David|   Miranda Grande|\n",
      "|cee04454-f6ea-48b...| 20|Lopez.Bernal@outl...| 7.35|   Laia|     Lopez Bernal|\n",
      "|6e5b75cd-0d5f-41f...| 22|MarcosySantiago@h...|  6.8| Marcos|     Garcia Aznar|\n",
      "|47435195-80b1-473...| 18|Judith.Garcia@gma...|  9.1| Judith|      Garcia Cruz|\n",
      "|fbdf66dc-49da-467...| 21| IkerySara@gmail.com| 7.77|   Iker|    Seco Coronado|\n",
      "|0df69140-84ac-47d...| 22|Lopez.Sastre@hotm...|  7.9|  Pablo|     Lopez Sastre|\n",
      "+--------------------+---+--------------------+-----+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Once the table is created you can access it using SQL as follows\n",
    "spark.sql('SELECT * FROM students LIMIT 10').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e6bb4f",
   "metadata": {},
   "source": [
    "### Understanding the Pyshical and Logical plans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a218f5b",
   "metadata": {},
   "source": [
    "As previously stated, the **Catalyst** is in charge of creating an optimizing the plan that will be converted into a DAG and sent to the executors. The following diagram shows the steps that the Catalyst performs to produce an execution plan\n",
    "\n",
    "<img src=\"../images/catalyst_steps.webp\" title=\"Catalyst Steps Diagram\" width=\"700px\"/>\n",
    "\n",
    "In order to see the plan generated by the **Catalyst** you can call the function `explain()` which, by default, will display the Physical Plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5ed09f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "CollectLimit 10\n",
      "+- FileScan json [_id#360,age#361L,email#362,grade#363,name#364,surname#365] Batched: false, DataFilters: [], Format: JSON, Location: InMemoryFileIndex(1 paths)[file:/home/amarchan/Documentos/formacion_numa/spark_NUMA_7/notebooks/d..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_id:string,age:bigint,email:string,grade:double,name:string,surname:string>\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "CollectLimit 10\n",
      "+- FileScan json [_id#360,age#361L,email#362,grade#363,name#364,surname#365] Batched: false, DataFilters: [], Format: JSON, Location: InMemoryFileIndex(1 paths)[file:/home/amarchan/Documentos/formacion_numa/spark_NUMA_7/notebooks/d..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_id:string,age:bigint,email:string,grade:double,name:string,surname:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(10).explain()\n",
    "spark.sql('SELECT * FROM students LIMIT 10').explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71202955",
   "metadata": {},
   "source": [
    "#### Getting various plans\n",
    "Before Apache Spark 3.0, there was only two modes available to format explain output.\n",
    "\n",
    "- `explain(extended=False)` which displayed only the physical plan\n",
    "- `explain(extended=True)` which displayed all the plans (logical and physical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "343a4f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "GlobalLimit 10\n",
      "+- LocalLimit 10\n",
      "   +- Relation [_id#360,age#361L,email#362,grade#363,name#364,surname#365] json\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "_id: string, age: bigint, email: string, grade: double, name: string, surname: string\n",
      "GlobalLimit 10\n",
      "+- LocalLimit 10\n",
      "   +- Relation [_id#360,age#361L,email#362,grade#363,name#364,surname#365] json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 10\n",
      "+- LocalLimit 10\n",
      "   +- Relation [_id#360,age#361L,email#362,grade#363,name#364,surname#365] json\n",
      "\n",
      "== Physical Plan ==\n",
      "CollectLimit 10\n",
      "+- FileScan json [_id#360,age#361L,email#362,grade#363,name#364,surname#365] Batched: false, DataFilters: [], Format: JSON, Location: InMemoryFileIndex(1 paths)[file:/home/amarchan/Documentos/formacion_numa/spark_NUMA_7/notebooks/d..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_id:string,age:bigint,email:string,grade:double,name:string,surname:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(10).explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb995b1",
   "metadata": {},
   "source": [
    "Starting from Apache Spark 3.0, you have a new parameter “mode” that produce expected format for the plan:\n",
    "\n",
    "- `explain(mode=\"simple\")` which will display the physical plan\n",
    "- `explain(mode=\"extended\")` which will display physical and logical plans (like \"extended\" option)\n",
    "- `explain(mode=\"codegen\")` which will display the java code planned to be executed\n",
    "- `explain(mode=\"cost\")` which will display the optimized logical plan and related statistics (if they exist)\n",
    "- `explain(mode=\"formatted\")` which will display a splitted output composed by a nice physical plan outline, and a section with each node details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "957536ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "CollectLimit (2)\n",
      "+- Scan json  (1)\n",
      "\n",
      "\n",
      "(1) Scan json \n",
      "Output [6]: [_id#360, age#361L, email#362, grade#363, name#364, surname#365]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/home/amarchan/Documentos/formacion_numa/spark_NUMA_7/notebooks/datasets/students.json]\n",
      "ReadSchema: struct<_id:string,age:bigint,email:string,grade:double,name:string,surname:string>\n",
      "\n",
      "(2) CollectLimit\n",
      "Input [6]: [_id#360, age#361L, email#362, grade#363, name#364, surname#365]\n",
      "Arguments: 10\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(10).explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa4a18e-a31e-431a-82a3-a3f330e61941",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Top 5 students with higher grades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a197fab-f130-492c-b6dc-afdff282221f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### DataFrame mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19b8d32d-dfc3-428f-a292-a1d86116782c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+--------------------+-----+-------+----------------+\n",
      "|                 _id|age|               email|grade|   name|         surname|\n",
      "+--------------------+---+--------------------+-----+-------+----------------+\n",
      "|9e27157e-062b-47f...| 22|LaiayVictoria@gma...| 10.0|   Laia|   Segovia Lopez|\n",
      "|1b36d010-64c0-4aa...| 23|Elena1993@outlook...| 9.99|  Elena| Sanchez Gisbert|\n",
      "|afae424a-3c2b-43f...| 25|Gomez.Segarra@hot...| 9.99|   Sara|   Gomez Segarra|\n",
      "|cc8f7297-5920-431...| 25|David.Pascual@yah...| 9.98|  David|Pascual Gonzalez|\n",
      "|8a4da0df-2988-4f9...| 19|Esteban1997@gmail...| 9.98|Esteban|   Comas Segovia|\n",
      "+--------------------+---+--------------------+-----+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "df.select('*').orderBy(desc('grade')).limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe725047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "TakeOrderedAndProject (2)\n",
      "+- Scan json  (1)\n",
      "\n",
      "\n",
      "(1) Scan json \n",
      "Output [6]: [_id#360, age#361L, email#362, grade#363, name#364, surname#365]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/home/amarchan/Documentos/formacion_numa/spark_NUMA_7/notebooks/datasets/students.json]\n",
      "ReadSchema: struct<_id:string,age:bigint,email:string,grade:double,name:string,surname:string>\n",
      "\n",
      "(2) TakeOrderedAndProject\n",
      "Input [6]: [_id#360, age#361L, email#362, grade#363, name#364, surname#365]\n",
      "Arguments: 5, [grade#363 DESC NULLS LAST], [_id#360, age#361L, email#362, grade#363, name#364, surname#365]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('*').orderBy(desc('grade')).limit(5).explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7decc916-e4d7-4281-a8ed-ff9e5f4dc50f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### SQL mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dbc009d5-4936-4fe7-953d-7865357e95f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+--------------------+-----+-------+----------------+\n",
      "|                 _id|age|               email|grade|   name|         surname|\n",
      "+--------------------+---+--------------------+-----+-------+----------------+\n",
      "|9e27157e-062b-47f...| 22|LaiayVictoria@gma...| 10.0|   Laia|   Segovia Lopez|\n",
      "|1b36d010-64c0-4aa...| 23|Elena1993@outlook...| 9.99|  Elena| Sanchez Gisbert|\n",
      "|afae424a-3c2b-43f...| 25|Gomez.Segarra@hot...| 9.99|   Sara|   Gomez Segarra|\n",
      "|cc8f7297-5920-431...| 25|David.Pascual@yah...| 9.98|  David|Pascual Gonzalez|\n",
      "|8a4da0df-2988-4f9...| 19|Esteban1997@gmail...| 9.98|Esteban|   Comas Segovia|\n",
      "+--------------------+---+--------------------+-----+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM students ORDER BY grade DESC LIMIT 5').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9687683b-9307-44e1-b249-e85ccece6611",
   "metadata": {},
   "source": [
    "### Max and Mean grade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16416cee-2fe0-42f6-946f-594822dfc9cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### DataFrame mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3e2143c-5c3e-49ae-9da5-3ecb671fdc30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+\n",
      "|max_grade|       mean_grade|\n",
      "+---------+-----------------+\n",
      "|     10.0|7.954802000000002|\n",
      "+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max, mean\n",
    "df.select(max('grade').alias('max_grade'), mean('grade').alias('mean_grade')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2714d0-12ae-4d8c-a309-6d46a25d84e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### SQL mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1764dc5a-4294-41a4-8eed-dfa20f74ffc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+\n",
      "|max_grade|       mean_grade|\n",
      "+---------+-----------------+\n",
      "|     10.0|7.954802000000002|\n",
      "+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    MAX(grade) AS max_grade,\n",
    "    MEAN(grade) AS mean_grade\n",
    "FROM\n",
    "    students\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d91a387-ebc6-44f8-bfd4-e64c0c09352e",
   "metadata": {},
   "source": [
    "### Top 5 students with highest mean grade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2810febf-15b6-4b7b-93f5-d2706383f8d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### DataFrame mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a5d041a-f608-4cf0-9b3c-67014b64c0b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+----------+\n",
      "|    name|         surname|mean_grade|\n",
      "+--------+----------------+----------+\n",
      "|    Laia|   Segovia Lopez|      10.0|\n",
      "|   Elena| Sanchez Gisbert|      9.99|\n",
      "| Esteban|   Comas Segovia|      9.98|\n",
      "|   David|Pascual Gonzalez|      9.98|\n",
      "|Fernando| Bermejo Gisbert|      9.98|\n",
      "+--------+----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "df.groupBy('name', 'surname') \\\n",
    "    .agg(mean('grade').alias('mean_grade')) \\\n",
    "    .orderBy(desc('mean_grade')) \\\n",
    "    .limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f62933-8d60-4889-883f-efb6747a5962",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### SQL mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ddcfad9-57c0-437e-8436-146031d22d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+----------+\n",
      "|    name|         surname|mean_grade|\n",
      "+--------+----------------+----------+\n",
      "|    Laia|   Segovia Lopez|      10.0|\n",
      "|   Elena| Sanchez Gisbert|      9.99|\n",
      "| Esteban|   Comas Segovia|      9.98|\n",
      "|   David|Pascual Gonzalez|      9.98|\n",
      "|Fernando| Bermejo Gisbert|      9.98|\n",
      "+--------+----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    name, \n",
    "    surname,\n",
    "    MEAN(grade) AS mean_grade\n",
    "FROM\n",
    "    students\n",
    "GROUP BY\n",
    "    name,\n",
    "    surname\n",
    "ORDER BY\n",
    "    mean_grade DESC\n",
    "LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68bf89a-186a-4c52-8314-f1883c7ab36b",
   "metadata": {},
   "source": [
    "### Average grade per student age"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d0443a-6d13-4e80-a9f1-39709223429c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### DataFrame mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14170333-adaa-480f-8113-75ee80b95123",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "|age|        mean_grade|\n",
      "+---+------------------+\n",
      "| 18| 7.935931528662416|\n",
      "| 19|7.9874188034187945|\n",
      "| 20| 7.920192791282483|\n",
      "| 21|7.9860449050086375|\n",
      "| 22| 7.916797804208606|\n",
      "| 23|7.9727236971484805|\n",
      "| 24| 7.999281364190009|\n",
      "| 25|7.9337354651162775|\n",
      "| 26| 7.903791102514502|\n",
      "| 27|  7.92304878048781|\n",
      "| 28| 7.921007194244603|\n",
      "| 29| 8.041525423728814|\n",
      "| 30| 8.114893617021275|\n",
      "| 31| 8.014242424242425|\n",
      "| 32|             8.091|\n",
      "| 33| 8.180588235294119|\n",
      "| 34|7.9399999999999995|\n",
      "| 35|             7.698|\n",
      "| 36|              7.68|\n",
      "| 37|              8.11|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "df.groupBy('age') \\\n",
    "    .agg(mean('grade').alias('mean_grade')) \\\n",
    "    .orderBy('age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7758d3cf-8e0c-46e9-9cf9-724cb47e3397",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### SQL mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3024835e-d24d-4897-9f85-5a35a9a1942b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "|age|        mean_grade|\n",
      "+---+------------------+\n",
      "| 29| 8.041525423728814|\n",
      "| 26| 7.903791102514502|\n",
      "| 19|7.9874188034187945|\n",
      "| 22| 7.916797804208606|\n",
      "| 34|7.9399999999999995|\n",
      "| 32|             8.091|\n",
      "| 31| 8.014242424242425|\n",
      "| 25|7.9337354651162775|\n",
      "| 27|  7.92304878048781|\n",
      "| 28| 7.921007194244603|\n",
      "| 33| 8.180588235294119|\n",
      "| 37|              8.11|\n",
      "| 35|             7.698|\n",
      "| 36|              7.68|\n",
      "| 18| 7.935931528662416|\n",
      "| 21|7.9860449050086375|\n",
      "| 30| 8.114893617021275|\n",
      "| 23|7.9727236971484805|\n",
      "| 20| 7.920192791282483|\n",
      "| 24| 7.999281364190009|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    age,\n",
    "    MEAN(grade) AS mean_grade\n",
    "FROM\n",
    "    students\n",
    "GROUP BY\n",
    "    age\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ca37c1-9301-41e0-9ba5-60aaccd42624",
   "metadata": {},
   "source": [
    "### Add column `excelent` to indicate that a student has a grade over 9.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab9c56a-9564-49d6-8607-1e49da312a70",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### DataFrame mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "52b350e3-fa38-4a0c-847d-72b72418c5d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+--------------------+-----+--------+-----------------+---------+\n",
      "|                 _id|age|               email|grade|    name|          surname|excellent|\n",
      "+--------------------+---+--------------------+-----+--------+-----------------+---------+\n",
      "|33a624e7-e6f1-40b...| 23|Valeria.Sebastian...| 7.56| Valeria| Sebastian Garcia|       NO|\n",
      "|2cd47675-43f3-415...| 23|Sanchez.Abascal@g...| 8.16|    Emma|  Sanchez Abascal|       NO|\n",
      "|594ea4e7-75e3-456...| 20|Sarabia.Lopez@gma...| 8.22| Agustin|    Sarabia Lopez|       NO|\n",
      "|3b521244-d2d4-40b...| 25|MartinaySebastian...| 7.67| Martina|Corominas Sarabia|       NO|\n",
      "|e6f52130-362f-4a5...| 19|DavidyValeria@gma...| 7.45|   David|   Miranda Grande|       NO|\n",
      "|cee04454-f6ea-48b...| 20|Lopez.Bernal@outl...| 7.35|    Laia|     Lopez Bernal|       NO|\n",
      "|6e5b75cd-0d5f-41f...| 22|MarcosySantiago@h...|  6.8|  Marcos|     Garcia Aznar|       NO|\n",
      "|47435195-80b1-473...| 18|Judith.Garcia@gma...|  9.1|  Judith|      Garcia Cruz|       NO|\n",
      "|fbdf66dc-49da-467...| 21| IkerySara@gmail.com| 7.77|    Iker|    Seco Coronado|       NO|\n",
      "|0df69140-84ac-47d...| 22|Lopez.Sastre@hotm...|  7.9|   Pablo|     Lopez Sastre|       NO|\n",
      "|dbd1026a-b18f-450...| 19|Marcos1997@gmail.com|  7.8|  Marcos|  Sarabia Gisbert|       NO|\n",
      "|ee2568cb-93f8-44c...| 24|  Oriol1992@yahoo.es| 7.39|   Oriol|   Blanes Miranda|       NO|\n",
      "|2f5239a1-05c0-454...| 18|Marcos.Cuenca@yah...|  9.8|  Marcos|     Cuenca Lopez|      YES|\n",
      "|40191301-f2b6-4aa...| 23|Sandra.Sastre@gma...|  8.3|  Sandra| Sastre Corominas|       NO|\n",
      "|eac69540-3a1d-4a3...| 22|LuciayTomas@outlo...| 8.69|   Lucia|      Agudo Lopez|       NO|\n",
      "|1f58ebe4-9fc5-429...| 29|MarcosyMario@gmai...|  8.8|  Marcos|   Garcia Sanchez|       NO|\n",
      "|54d4a8d3-4cc1-4dc...| 24|EmmayJose@hotmail.es| 9.25|    Emma|     Cruz Antunez|       NO|\n",
      "|33f54c17-2a1b-421...| 21|Santander.Pascual...| 5.79|   Pedro|Santander Pascual|       NO|\n",
      "|971deb27-156d-4cd...| 21|Aznar.Pascual@idx...|  8.6|Santiago|    Aznar Pascual|       NO|\n",
      "|cb1244e2-63cf-4a7...| 20|Juan.Parada@gmail...|  8.3|    Juan|   Parada Pascual|       NO|\n",
      "+--------------------+---+--------------------+-----+--------+-----------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "df.withColumn('excellent', when(col('grade') > 9.5, 'YES').otherwise('NO')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31b22fe-a23c-4da3-acf3-2eb9cce7a2ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### SQL mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50dfa5b6-0d51-4564-87d9-78bcdfd1fd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+--------------------+-----+--------+-----------------+---------+\n",
      "|                 _id|age|               email|grade|    name|          surname|excellent|\n",
      "+--------------------+---+--------------------+-----+--------+-----------------+---------+\n",
      "|33a624e7-e6f1-40b...| 23|Valeria.Sebastian...| 7.56| Valeria| Sebastian Garcia|       NO|\n",
      "|2cd47675-43f3-415...| 23|Sanchez.Abascal@g...| 8.16|    Emma|  Sanchez Abascal|       NO|\n",
      "|594ea4e7-75e3-456...| 20|Sarabia.Lopez@gma...| 8.22| Agustin|    Sarabia Lopez|       NO|\n",
      "|3b521244-d2d4-40b...| 25|MartinaySebastian...| 7.67| Martina|Corominas Sarabia|       NO|\n",
      "|e6f52130-362f-4a5...| 19|DavidyValeria@gma...| 7.45|   David|   Miranda Grande|       NO|\n",
      "|cee04454-f6ea-48b...| 20|Lopez.Bernal@outl...| 7.35|    Laia|     Lopez Bernal|       NO|\n",
      "|6e5b75cd-0d5f-41f...| 22|MarcosySantiago@h...|  6.8|  Marcos|     Garcia Aznar|       NO|\n",
      "|47435195-80b1-473...| 18|Judith.Garcia@gma...|  9.1|  Judith|      Garcia Cruz|       NO|\n",
      "|fbdf66dc-49da-467...| 21| IkerySara@gmail.com| 7.77|    Iker|    Seco Coronado|       NO|\n",
      "|0df69140-84ac-47d...| 22|Lopez.Sastre@hotm...|  7.9|   Pablo|     Lopez Sastre|       NO|\n",
      "|dbd1026a-b18f-450...| 19|Marcos1997@gmail.com|  7.8|  Marcos|  Sarabia Gisbert|       NO|\n",
      "|ee2568cb-93f8-44c...| 24|  Oriol1992@yahoo.es| 7.39|   Oriol|   Blanes Miranda|       NO|\n",
      "|2f5239a1-05c0-454...| 18|Marcos.Cuenca@yah...|  9.8|  Marcos|     Cuenca Lopez|      YES|\n",
      "|40191301-f2b6-4aa...| 23|Sandra.Sastre@gma...|  8.3|  Sandra| Sastre Corominas|       NO|\n",
      "|eac69540-3a1d-4a3...| 22|LuciayTomas@outlo...| 8.69|   Lucia|      Agudo Lopez|       NO|\n",
      "|1f58ebe4-9fc5-429...| 29|MarcosyMario@gmai...|  8.8|  Marcos|   Garcia Sanchez|       NO|\n",
      "|54d4a8d3-4cc1-4dc...| 24|EmmayJose@hotmail.es| 9.25|    Emma|     Cruz Antunez|       NO|\n",
      "|33f54c17-2a1b-421...| 21|Santander.Pascual...| 5.79|   Pedro|Santander Pascual|       NO|\n",
      "|971deb27-156d-4cd...| 21|Aznar.Pascual@idx...|  8.6|Santiago|    Aznar Pascual|       NO|\n",
      "|cb1244e2-63cf-4a7...| 20|Juan.Parada@gmail...|  8.3|    Juan|   Parada Pascual|       NO|\n",
      "+--------------------+---+--------------------+-----+--------+-----------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    *,\n",
    "    CASE \n",
    "        WHEN grade > 9.5 THEN 'YES'\n",
    "        ELSE 'NO'\n",
    "    END AS excellent\n",
    "FROM\n",
    "    students\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5edddc7-3990-4856-81bb-6f7baea617d1",
   "metadata": {},
   "source": [
    "## 4. Spark SQL Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aeb6b6-d5d7-42b0-97cb-73c39cb4b496",
   "metadata": {},
   "source": [
    "Spark DataFrame supports all basic SQL Join Types like `INNER`, `LEFT OUTER`, `RIGHT OUTER`, `LEFT ANTI`, `LEFT SEMI`, `CROSS`, `FULL OUTER`, etc. Spark SQL Joins are wide transformations that result in data shuffling over the network hence they have huge performance issues when not designed with care.\n",
    "\n",
    "On the other hand Spark SQL Joins comes with more optimization by default (thanks to DataFrames & Dataset) however still there would be some performance issues to consider while using them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1388506-e203-49cd-ab5a-81fd88dc2d9d",
   "metadata": {},
   "source": [
    "### Most common types of Joins\n",
    "\n",
    "<img src=\"../images/joins.jpg\" title=\"Types of Joins\" width=\"700px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c79a759-6e25-469a-b5f9-951959e7601d",
   "metadata": {},
   "source": [
    "### How to perform a Join in Spark (cheat sheet)\n",
    "\n",
    "1. Referencing columns with different names in each df:\n",
    "\n",
    "```python\n",
    "dfA.join(dfB, dfA[\"idA\"] == dfB[\"idB\"], \"type-of-join\")\n",
    "\n",
    "```\n",
    "\n",
    "2. Key column has the same alias\n",
    "\n",
    "```python\n",
    "dfA.join(dfB, on=\"id\", how=\"type-of-join\")\n",
    "\n",
    "```\n",
    "\n",
    "3. More than one key column\n",
    "\n",
    "```python\n",
    "dfA.join(dfB, [\"id\", \"code\"], \"type-of-join\")\n",
    "\n",
    "dfA.join(dfB, (dfA[\"id\"] == dfB[\"id\"]) & (dfA[\"code\"] == dfB[\"code\"]), \"type-of-join\")\n",
    "\n",
    "```\n",
    "\n",
    "4. Maintaining both columns (be careful and be sure of referencing the origin of the column when using it after this)\n",
    "\n",
    "```python\n",
    "dfC = dfA.join(dfB, dfA[\"id\"] == dfB[\"id\"], \"type-of-join\")\n",
    "\n",
    "dfC.select(dfA[\"id\"]).show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd9c5ad-43ee-492a-9eb8-dee1f6ed56ee",
   "metadata": {},
   "source": [
    "### Read the example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5daaf593-e0c0-4332-a47e-41514847ac8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------+--------+------+---------------+-----------+\n",
      "|emp_dept_id|emp_id|gender|    name|salary|superior_emp_id|year_joined|\n",
      "+-----------+------+------+--------+------+---------------+-----------+\n",
      "|         10|     1|     M|   Smith|  3000|             -1|       2018|\n",
      "|         20|     2|     M|    Rose|  4000|              1|       2010|\n",
      "|         10|     4|     F|   Jones|  2000|              2|       2005|\n",
      "|         10|     3|     M|Williams|  1000|              1|       2010|\n",
      "|         50|     6|      |   Brown|    -1|              2|       2010|\n",
      "|         40|     5|      |   Brown|    -1|              2|       2010|\n",
      "+-----------+------+------+--------+------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp = spark.read.json(\"../datasets/employees.json\")\n",
    "df_emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3adacfc8-e829-4052-9770-41ec7fd9dfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|dept_id|dept_name|\n",
      "+-------+---------+\n",
      "|     10|  Finance|\n",
      "|     20|Marketing|\n",
      "|     30|    Sales|\n",
      "|     40|       IT|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dept = spark.read.json(\"../datasets/departments.json\")\n",
    "df_dept.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e03c03-683f-478b-84a7-c7d0179f70d0",
   "metadata": {},
   "source": [
    "### Inner Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b97e7753-a993-411f-bfa8-c896dec9d28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------+--------+------+---------------+-----------+-------+---------+\n",
      "|emp_dept_id|emp_id|gender|    name|salary|superior_emp_id|year_joined|dept_id|dept_name|\n",
      "+-----------+------+------+--------+------+---------------+-----------+-------+---------+\n",
      "|         10|     1|     M|   Smith|  3000|             -1|       2018|     10|  Finance|\n",
      "|         20|     2|     M|    Rose|  4000|              1|       2010|     20|Marketing|\n",
      "|         10|     4|     F|   Jones|  2000|              2|       2005|     10|  Finance|\n",
      "|         10|     3|     M|Williams|  1000|              1|       2010|     10|  Finance|\n",
      "|         40|     5|      |   Brown|    -1|              2|       2010|     40|       IT|\n",
      "+-----------+------+------+--------+------+---------------+-----------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df_emp.join(df_dept,  df_emp[\"emp_dept_id\"] == df_dept[\"dept_id\"], how=\"inner\").show()\n",
    "df_emp.join(df_dept, df_emp[\"emp_dept_id\"] == df_dept[\"dept_id\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154f72a3-6a63-4ec2-a4bb-81a8989f6a07",
   "metadata": {},
   "source": [
    "- The employee that belongs to inexistent department with id 50 is dropped\n",
    "- The Sales department that has no employees asociated is dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51172a5-7a61-4f0c-bcf3-889044cceac8",
   "metadata": {},
   "source": [
    "### Full Outer Join\n",
    "\n",
    "Outer a.k.a full, fullouter join returns all rows from both Spark DataFrame, where join expression doesn’t match it returns null on respective record columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "129a4cfe-8690-4b0c-8e80-9c71061c63cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------+--------+------+---------------+-----------+-------+---------+\n",
      "|emp_dept_id|emp_id|gender|    name|salary|superior_emp_id|year_joined|dept_id|dept_name|\n",
      "+-----------+------+------+--------+------+---------------+-----------+-------+---------+\n",
      "|         10|     1|     M|   Smith|  3000|             -1|       2018|     10|  Finance|\n",
      "|         10|     4|     F|   Jones|  2000|              2|       2005|     10|  Finance|\n",
      "|         10|     3|     M|Williams|  1000|              1|       2010|     10|  Finance|\n",
      "|         20|     2|     M|    Rose|  4000|              1|       2010|     20|Marketing|\n",
      "|       null|  null|  null|    null|  null|           null|       null|     30|    Sales|\n",
      "|         40|     5|      |   Brown|    -1|              2|       2010|     40|       IT|\n",
      "|         50|     6|      |   Brown|    -1|              2|       2010|   null|     null|\n",
      "+-----------+------+------+--------+------+---------------+-----------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.join(df_dept, df_emp[\"emp_dept_id\"] == df_dept[\"dept_id\"],\"outer\").show()\n",
    "# df_emp.join(df_dept, df_emp[\"emp_dept_id\"] == df_dept[\"dept_id\"],\"fullouter\").show()\n",
    "# df_emp.join(df_dept, df_emp[\"emp_dept_id\"] == df_dept[\"dept_id\"],\"full\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5848c1-d807-475b-a96e-d37d460031a2",
   "metadata": {},
   "source": [
    "- There is no department with id 50 hence the values for the department are null\n",
    "- There are no employees in the Sales department hence the employee values are null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f5335d-b732-4ce7-9e64-e3a825ba6cc5",
   "metadata": {},
   "source": [
    "### Left Join\n",
    "\n",
    "Left a.k.a Left Outer join returns all rows from the left DataFrame/Dataset regardless of match found on the right dataset when join expression doesn’t match, it assigns null for that record and drops records from right where match not found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c7215ac3-e9a7-483e-a32c-fc10fb51c2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------+--------+------+---------------+-----------+-------+---------+\n",
      "|emp_dept_id|emp_id|gender|    name|salary|superior_emp_id|year_joined|dept_id|dept_name|\n",
      "+-----------+------+------+--------+------+---------------+-----------+-------+---------+\n",
      "|         10|     1|     M|   Smith|  3000|             -1|       2018|     10|  Finance|\n",
      "|         20|     2|     M|    Rose|  4000|              1|       2010|     20|Marketing|\n",
      "|         10|     4|     F|   Jones|  2000|              2|       2005|     10|  Finance|\n",
      "|         10|     3|     M|Williams|  1000|              1|       2010|     10|  Finance|\n",
      "|         50|     6|      |   Brown|    -1|              2|       2010|   null|     null|\n",
      "|         40|     5|      |   Brown|    -1|              2|       2010|     40|       IT|\n",
      "+-----------+------+------+--------+------+---------------+-----------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.join(df_dept, df_emp[\"emp_dept_id\"] == df_dept[\"dept_id\"],\"left\").show()\n",
    "# df_emp.join(df_dept, df_emp[\"emp_dept_id\"] == df_dept[\"dept_id\"],\"leftouter\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327fe8a8-84ca-4ccc-9b7c-59eef5bfb81c",
   "metadata": {},
   "source": [
    "- There is no department with id 50 hence the values for the department are null\n",
    "- There are no employees in the Sales department hence no row is shown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ffa2d4-963d-4701-87d9-f1700bff149e",
   "metadata": {},
   "source": [
    "### Right Join\n",
    "\n",
    "Right a.k.a Right Outer join is opposite to left join, here it returns all rows from the right DataFrame/Dataset regardless of match found on the left dataset, when join expression doesn’t match, it assigns null for that record and drops records from left where match not found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fa185ac6-3135-4b1c-88dd-483243a3f548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------+--------+------+---------------+-----------+-------+---------+\n",
      "|emp_dept_id|emp_id|gender|    name|salary|superior_emp_id|year_joined|dept_id|dept_name|\n",
      "+-----------+------+------+--------+------+---------------+-----------+-------+---------+\n",
      "|         10|     3|     M|Williams|  1000|              1|       2010|     10|  Finance|\n",
      "|         10|     4|     F|   Jones|  2000|              2|       2005|     10|  Finance|\n",
      "|         10|     1|     M|   Smith|  3000|             -1|       2018|     10|  Finance|\n",
      "|         20|     2|     M|    Rose|  4000|              1|       2010|     20|Marketing|\n",
      "|       null|  null|  null|    null|  null|           null|       null|     30|    Sales|\n",
      "|         40|     5|      |   Brown|    -1|              2|       2010|     40|       IT|\n",
      "+-----------+------+------+--------+------+---------------+-----------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.join(df_dept, df_emp[\"emp_dept_id\"] == df_dept[\"dept_id\"],\"right\").show()\n",
    "# df_emp.join(df_dept, df_emp[\"emp_dept_id\"] == df_dept[\"dept_id\"],\"rightouter\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89fcaf6-0350-40ff-8a62-92514160ccbc",
   "metadata": {},
   "source": [
    "- There is no department with id 50 hence no row is shown\n",
    "- There are no employees in the Sales department hence the values of the employee columns are null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f215108-ed56-446f-816e-dc6c8c1962c4",
   "metadata": {},
   "source": [
    "### CrossJoin\n",
    "\n",
    "This join combines each row of the first table with each row of the second table. For example, we have `m` rows in one table and `n` rows in another, this gives us `m * n` rows in the resulting table. \n",
    "\n",
    "`Note`: A table of 1000 customers combined with a table of 1000 products would produce 1,000,000 records! Try to avoid this with large tables in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ac1a776b-66e7-4cd2-91c3-330e524be475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------+--------+------+---------------+-----------+-------+---------+\n",
      "|emp_dept_id|emp_id|gender|    name|salary|superior_emp_id|year_joined|dept_id|dept_name|\n",
      "+-----------+------+------+--------+------+---------------+-----------+-------+---------+\n",
      "|         10|     1|     M|   Smith|  3000|             -1|       2018|     10|  Finance|\n",
      "|         10|     1|     M|   Smith|  3000|             -1|       2018|     20|Marketing|\n",
      "|         10|     1|     M|   Smith|  3000|             -1|       2018|     30|    Sales|\n",
      "|         10|     1|     M|   Smith|  3000|             -1|       2018|     40|       IT|\n",
      "|         20|     2|     M|    Rose|  4000|              1|       2010|     10|  Finance|\n",
      "|         20|     2|     M|    Rose|  4000|              1|       2010|     20|Marketing|\n",
      "|         20|     2|     M|    Rose|  4000|              1|       2010|     30|    Sales|\n",
      "|         20|     2|     M|    Rose|  4000|              1|       2010|     40|       IT|\n",
      "|         10|     4|     F|   Jones|  2000|              2|       2005|     10|  Finance|\n",
      "|         10|     4|     F|   Jones|  2000|              2|       2005|     20|Marketing|\n",
      "|         10|     4|     F|   Jones|  2000|              2|       2005|     30|    Sales|\n",
      "|         10|     4|     F|   Jones|  2000|              2|       2005|     40|       IT|\n",
      "|         10|     3|     M|Williams|  1000|              1|       2010|     10|  Finance|\n",
      "|         10|     3|     M|Williams|  1000|              1|       2010|     20|Marketing|\n",
      "|         10|     3|     M|Williams|  1000|              1|       2010|     30|    Sales|\n",
      "|         10|     3|     M|Williams|  1000|              1|       2010|     40|       IT|\n",
      "|         50|     6|      |   Brown|    -1|              2|       2010|     10|  Finance|\n",
      "|         50|     6|      |   Brown|    -1|              2|       2010|     20|Marketing|\n",
      "|         50|     6|      |   Brown|    -1|              2|       2010|     30|    Sales|\n",
      "|         50|     6|      |   Brown|    -1|              2|       2010|     40|       IT|\n",
      "+-----------+------+------+--------+------+---------------+-----------+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.crossJoin(df_dept).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1443e6da",
   "metadata": {},
   "source": [
    "## 5. User Defined Functions (UDFs)\n",
    "User Defined Functions (UDFs) in Spark allow users to define their own transformations using Python or other programming languages, and then apply those functions on a Spark DataFrame. This can be very powerful when you need to make specific transformations to your data that aren't easily achieved using Spark's built-in functions.\n",
    "\n",
    "When a UDF is defined, under the hood, Spark serializes the function using Py4J, transfers it over to the executor nodes, and deserializes it. This allows the UDF to be executed on rows of the DataFrame in parallel. However, it's worth noting that because UDFs involve serialization and data transfer between Python and JVM, they can be considerably slower than using native Spark functions.\n",
    "\n",
    "Here's how you can define and use a UDF in Spark with Python (PySpark):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0064e63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def add_suffix(name):\n",
    "    return name + \"_UDF\"\n",
    "\n",
    "suffix_udf = udf(add_suffix, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9f1556",
   "metadata": {},
   "source": [
    "#### DataFrame mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aaa09949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n",
      "|    name|name_with_suffix|\n",
      "+--------+----------------+\n",
      "| Valeria|     Valeria_UDF|\n",
      "|    Emma|        Emma_UDF|\n",
      "| Agustin|     Agustin_UDF|\n",
      "| Martina|     Martina_UDF|\n",
      "|   David|       David_UDF|\n",
      "|    Laia|        Laia_UDF|\n",
      "|  Marcos|      Marcos_UDF|\n",
      "|  Judith|      Judith_UDF|\n",
      "|    Iker|        Iker_UDF|\n",
      "|   Pablo|       Pablo_UDF|\n",
      "|  Marcos|      Marcos_UDF|\n",
      "|   Oriol|       Oriol_UDF|\n",
      "|  Marcos|      Marcos_UDF|\n",
      "|  Sandra|      Sandra_UDF|\n",
      "|   Lucia|       Lucia_UDF|\n",
      "|  Marcos|      Marcos_UDF|\n",
      "|    Emma|        Emma_UDF|\n",
      "|   Pedro|       Pedro_UDF|\n",
      "|Santiago|    Santiago_UDF|\n",
      "|    Juan|        Juan_UDF|\n",
      "+--------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"name_with_suffix\", suffix_udf(df[\"name\"])) \\\n",
    "  .select(\"name\", \"name_with_suffix\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4175a7",
   "metadata": {},
   "source": [
    "#### SQL mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "805f61d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n",
      "|    name|name_with_suffix|\n",
      "+--------+----------------+\n",
      "| Valeria|     Valeria_UDF|\n",
      "|    Emma|        Emma_UDF|\n",
      "| Agustin|     Agustin_UDF|\n",
      "| Martina|     Martina_UDF|\n",
      "|   David|       David_UDF|\n",
      "|    Laia|        Laia_UDF|\n",
      "|  Marcos|      Marcos_UDF|\n",
      "|  Judith|      Judith_UDF|\n",
      "|    Iker|        Iker_UDF|\n",
      "|   Pablo|       Pablo_UDF|\n",
      "|  Marcos|      Marcos_UDF|\n",
      "|   Oriol|       Oriol_UDF|\n",
      "|  Marcos|      Marcos_UDF|\n",
      "|  Sandra|      Sandra_UDF|\n",
      "|   Lucia|       Lucia_UDF|\n",
      "|  Marcos|      Marcos_UDF|\n",
      "|    Emma|        Emma_UDF|\n",
      "|   Pedro|       Pedro_UDF|\n",
      "|Santiago|    Santiago_UDF|\n",
      "|    Juan|        Juan_UDF|\n",
      "+--------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register the UDF\n",
    "spark.udf.register(\"suffixSQL\", add_suffix, StringType())\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    name,\n",
    "    suffixSQL(name) as name_with_suffix\n",
    "FROM\n",
    "    students\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
