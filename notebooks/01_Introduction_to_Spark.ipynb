{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "696bf464-58f3-4280-8309-5bb100e6856b",
   "metadata": {},
   "source": [
    "# Introduction to Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2afb38c-8767-4c31-a396-2480d14a2512",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. What is Apache Spark?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5f3b69-d579-4738-966c-21eaba0a9c32",
   "metadata": {},
   "source": [
    "Apache Spark is an **open-source distributed computing system** that provides an interface for programming entire clusters with implicit **data parallelism** and **fault tolerance**. It was built on top of **Hadoop MapReduce** and extends the MapReduce model to efficiently use more types of computations, including interactive queries and stream processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8216f5-e0c4-40fb-bf1c-034188a763f2",
   "metadata": {},
   "source": [
    "**Key aspects**:\n",
    "- Framework for distributed data computing.\n",
    "- Designed to be executed in large scale clusters with lots of data!\n",
    "- Run faster than Hadoop MapReduce, up to 100x due to memory (RAM) usage.\n",
    "- More functions than just Map and Reduce.\n",
    "- Multiple APIs, multiple programming languages:\n",
    "    - Core, SQL, Streaming, GraphX, ML, MlLib, Structured Streaming, …\n",
    "    - Scala (native), Java (native?), Python, R.\n",
    "- Runs everywhere:\n",
    "    - Standalone, YARN, Mesos, Kubernetes, AWS, ...\n",
    "- Fault tolerance (RDD).\n",
    "- Easier resource managing.\n",
    "- Reusable data: caching.\n",
    "- Code control and analysis (DAG).\n",
    "- Generic programming patterns: the same code can run in local mode or 100’s of executors.\n",
    "- Lazy evaluation: transformations and actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed972e6b-f4d9-401d-920a-2ae1fde10a93",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1 Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300911a6-a23c-43e2-83d3-91428c82e852",
   "metadata": {},
   "source": [
    "Apache Spark is a distributed data processing system, and its architecture is designed to efficiently distribute and process data across a cluster of computers. Here's a simple explanation of its main components:\n",
    "\n",
    "**1. Driver Program**: Think of this as the \"master\" of the application. It's where your Spark application starts and where the final results are collected. It's responsible for:\n",
    "- Running the main function of the application.\n",
    "- Creating the SparkContext to coordinate tasks.\n",
    "- Distributing tasks across executor processes.\n",
    "\n",
    "**2. SparkContext (SC)**: This is like the \"brain\" of your Spark application. Once initialized, it coordinates tasks and keeps a connection with the Spark cluster.\n",
    "\n",
    "**3. Cluster Manager**: This can be likened to a \"job dispatcher\". It's not part of Spark per se, but Spark can work with several of them, like Apache Mesos, Hadoop YARN, Kubernetes, or the built-in standalone manager. Its job is to:\n",
    "- Allocate resources (like memory and CPU) for Spark applications.\n",
    "- Keep track of available/used resources.\n",
    "  \n",
    "**4. Executors**: These are like the \"workers\". Each executor:\n",
    "- Runs on a node in the Spark cluster.\n",
    "- Is responsible for executing the tasks assigned by the driver program.\n",
    "- Stores data in its memory for quick access.\n",
    "   \n",
    "**5. Tasks**: These are the \"actual work\" that needs to be done. When you write a Spark application, the driver breaks down the operations into tasks that are sent to the executors. Each task:\n",
    "- Works on a slice of your data.\n",
    "- Runs on an executor.\n",
    "\n",
    "![Spark Architecture Diagram](images/spark_architecture.png)\n",
    "\n",
    "**6. Jobs, Stages, and Tasks**: \n",
    "- **Job**: A complete computation, which can be a single action like `count()` or `saveAsTextFile()`.\n",
    "- **Stage**: Jobs are divided into stages. A stage is a set of transformations on data (e.g., filtering, mapping). Stages are divided based on transformations that have wide dependencies (like a reduce) which often involve shuffling data around.\n",
    "- **Task**: Each stage is further divided into tasks. A task is a unit of work sent to an executor.\n",
    "\n",
    "<img src=\"images/jobs_stages_tasks.png\" title=\"Jobs, Stages, and Tasks\" width=\"700px\"/>\n",
    "\n",
    "**7. RDD (Resilient Distributed Dataset)**: Think of these as the \"backbone\" of data in Spark. They are:\n",
    "- Immutable distributed collections of data.\n",
    "- Split into partitions, with each partition residing on a single node.\n",
    "- Can be cached in memory for faster access.\n",
    "\n",
    "**8. DataFrames and Datasets**: These are like \"enhanced\" RDDs. They:\n",
    "- Offer more optimizations.\n",
    "- Come with a schema, so you can think of them as distributed tables.\n",
    "\n",
    "In essence, when you run a Spark application, the driver program coordinates tasks to be executed. The cluster manager allocates resources, and executors on various nodes run these tasks on slices of data. The processed data can be collected back to the driver or stored in external storage systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437b4dcb-6c3b-4488-a609-a0a38f47f81b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Components\n",
    "\n",
    "Apache Spark has a well-defined layered architecture where all the Spark components and layers are loosely coupled. This architecture is further integrated with various extensions and libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb24a4d-bfc0-43c1-b3b9-2ab03357c26c",
   "metadata": {},
   "source": [
    "<img src=\"images/spark_stack.png\" title=\"The Spark Stack\" width=\"600px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e3fad9-cbf7-4454-b3ca-b094a739b864",
   "metadata": {},
   "source": [
    "### 2.1 Spark Core\n",
    "\n",
    "Spark Core is the underlying general execution engine. It provides in-memory computing capabilities to deliver speed, a generalized execution model, and the ability to integrate with a wide variety of data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33548919-3687-4200-8a5d-42b752cf023a",
   "metadata": {},
   "source": [
    "### 2.2 Spark SQL\n",
    "\n",
    "Spark SQL is Spark's package for working with structured data. It provides a programming interface for data structured as well as relational processing with SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f14fb0-84ce-4a68-950f-b0ba826aace6",
   "metadata": {},
   "source": [
    "### 2.3 Spark Streaming\n",
    "\n",
    "Spark Streaming allows the processing of live data streams. With Spark Streaming, you can use Spark's API for processing data and then use the same code to process real-time data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a56f6b7-95e2-49d3-b192-7ac10a881268",
   "metadata": {},
   "source": [
    "### 2.4 MLlib\n",
    "\n",
    "MLlib is Spark's machine learning (ML) library. It provides multiple types of machine learning algorithms, including classification, regression, clustering, and collaborative filtering, as well as tools for constructing, evaluating, and tuning ML pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f4ecee-a011-4465-9f76-ec83a28f801f",
   "metadata": {},
   "source": [
    "### 2.5 GraphX\n",
    "\n",
    "GraphX is Spark's API for graphs and graph-parallel computation. It provides a growing library of graph algorithms and builders to simplify graph analytics tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab81506-2ae9-4921-b23d-a2949bae42fc",
   "metadata": {},
   "source": [
    "## 3. Creating our first Spark Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc58213-c07e-4777-8154-9ab74959961a",
   "metadata": {},
   "source": [
    "In Jupyter notebooks you can check the documentation of Python functions and classes prepending the `?` and `??` directives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b13489-c3fc-47b0-9ef4-7a444f7b5e3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "?SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f78a743-35c6-4ae4-a908-e80470ec176c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "            .appName(\"My first Spark Session\") \\\n",
    "            .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3b7db3-f9c4-4ed0-a4ec-5f39b241852a",
   "metadata": {},
   "source": [
    "The Spark UI link provides you a graphical interface to monitor information about the processes running in your Spark Session, such as Jobs, Stages, Executors and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30974ad-fd89-4909-a9b3-e9c7ec55bc06",
   "metadata": {},
   "source": [
    "## 4. Understanding RDDs (Resilient Distributed Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f0970b-29b3-4e1a-b845-b63862c5beb7",
   "metadata": {},
   "source": [
    "RDD is a fundamental data structure of Spark. It is an immutable distributed collection of objects that can be processed in parallel. RDDs can be created from Hadoop InputFormats or by transforming other RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13193ad6-d8a5-4635-a19f-d01e09bc7c3c",
   "metadata": {},
   "source": [
    "### Properties of RDD:\n",
    "- **Immutable**: Once created, the data they contain cannot be changed.\n",
    "- **Lazy Evaluations**: Computations on RDDs are lazily evaluated, meaning that tasks are not executed until an action is called.\n",
    "- **Fault Tolerant**: They track data lineage information to rebuild lost data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f6cb02-34dd-47dc-813c-72c7c03babfe",
   "metadata": {},
   "source": [
    "### Creating a RDD:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e432e2f-b0ab-4310-873e-26b155dd3719",
   "metadata": {},
   "source": [
    "RDDs can be created in two ways: by loading an external dataset or by distributing a set of collection objects (like lists or sets) from the driver program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb3438a-8fb8-47ff-913a-c5dd499990bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# From a list\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "# From a file\n",
    "rdd_from_file = spark.sparkContext.textFile(\"datasets/numbers.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19a1203-d492-4377-8338-f5bef5a0eb4f",
   "metadata": {},
   "source": [
    "<img src=\"images/RDD_concept.png\" title=\"RDD concept\" width=\"700px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1941c1-766e-4188-9bc9-f7ced59dd59c",
   "metadata": {},
   "source": [
    "- The `parallelize` function distributes the data in partitions across the executors in the cluster and returns a RDD.\n",
    "- The `collect` function retrieves all the partitions into the driver which then returns a single Python collection.\n",
    "\n",
    "`Warning`: Using collect on a large RDD can be problematic as it brings the entire dataset to the driver, possibly causing it to run out of memory. It's generally used with caution and mainly for retrieving small results or during debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e4053e-478b-4842-ade7-8c7fe9417d48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c517554-eb17-4fbb-a6d4-0b8cc9e65ecb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rdd_from_file.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b17901-3486-4007-bb4f-9bbd61272e7b",
   "metadata": {},
   "source": [
    "The `glom()` function returns a list of the elements in each partition of the RDD as you can see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885be32e-2b0f-4819-9b83-c45491f771ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4e8536-5316-49b4-9156-73e13f92ee6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rdd.repartition(3).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d660ebd-36d4-4cdb-b5d7-876a6d1b5451",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c4b6b-f944-4377-ae5c-ae6960ca5d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6ee7ed-c473-49fd-b109-f5c94c8b934c",
   "metadata": {},
   "source": [
    "Execute the following cell and hide the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04141367-1c63-4efd-9b39-a1f096767bb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "style = {'description_width': 'initial', 'width': '500px'}\n",
    "\n",
    "OUTPUT = widgets.Output()\n",
    "\n",
    "def create_question(index, question, options, answer_idx):\n",
    "    is_multiple = isinstance(answer_idx, list)\n",
    "\n",
    "    question_dropdown = widgets.RadioButtons(\n",
    "        options=options,\n",
    "        description=f'{index}. {question}',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='100%'),\n",
    "        style=style\n",
    "    ) if not is_multiple else widgets.SelectMultiple(\n",
    "        options=options,\n",
    "        description=f'{index}. {question} (Choose all that apply by clicking while holding CTRL)',\n",
    "        disabled=False,\n",
    "        layout=widgets.Layout(width='100%'),\n",
    "        style=style\n",
    "    )\n",
    "\n",
    "    check_button = widgets.Button(description=f\"Check question {index}\")\n",
    "\n",
    "    output = widgets.Output()\n",
    "\n",
    "    def check_answer(button):\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            if(is_multiple):\n",
    "                if sorted(question_dropdown.index) == sorted(answer_idx):\n",
    "                    print(f\"Question {index} is correct! 🎉\")\n",
    "                else:\n",
    "                    print(f\"Question {index} is incorrect 😢 Rigth answer is {', '.join([options[idx] for idx in answer_idx])}\")\n",
    "                return\n",
    "            \n",
    "            if question_dropdown.value == options[answer_idx]:\n",
    "                print(f\"Question {index} is correct! 🎉\")\n",
    "            else:\n",
    "                print(f\"Question {index} is incorrect. 😢 Rigth answer is {options[answer_idx]}\")\n",
    "\n",
    "    check_button.on_click(check_answer)\n",
    "    \n",
    "    return question_dropdown, check_button, output\n",
    "\n",
    "questions = [\n",
    "    {\n",
    "        \"question\": \"What is Apache Spark?\",\n",
    "        \"options\": [\n",
    "            \"A proprietary distributed computing system.\",\n",
    "            \"An open-source machine learning framework.\",\n",
    "            \"A graph database system.\",\n",
    "            \"An open-source distributed computing system.\"\n",
    "        ],\n",
    "        \"answer_idx\": 3\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which of the following is NOT a key aspect of Apache Spark?\",\n",
    "        \"options\": [\n",
    "            \"Framework for distributed data computing.\",\n",
    "            \"Designed to be executed in large scale clusters with lots of data.\",\n",
    "            \"Multi-threaded in-memory database.\",\n",
    "            \"Code control and analysis (DAG).\"\n",
    "        ],\n",
    "        \"answer_idx\": 2\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What does the Driver Program in Spark's architecture do?\",\n",
    "        \"options\": [\n",
    "            \"It is responsible for executing the tasks assigned by the executors.\",\n",
    "            \"Runs the main function of the application.\",\n",
    "            \"Allocates resources (like memory and CPU) for Spark applications.\",\n",
    "            \"Works on a slice of your data.\"\n",
    "        ],\n",
    "        \"answer_idx\": 1\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which Cluster Manager is NOT supported by Apache Spark?\",\n",
    "        \"options\": [\n",
    "            \"Apache Mesos\",\n",
    "            \"Hadoop YARN\",\n",
    "            \"Apache Kafka\",\n",
    "            \"Kubernetes\"\n",
    "        ],\n",
    "        \"answer_idx\": 2\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Which Spark component is responsible for processing live data streams?\",\n",
    "        \"options\": [\n",
    "            \"Spark Core\",\n",
    "            \"Spark Streaming\",\n",
    "            \"Spark SQL\",\n",
    "            \"MLlib\"\n",
    "        ],\n",
    "        \"answer_idx\": 1\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are the properties of RDD?\",\n",
    "        \"options\": [\n",
    "            \"Immutable\",\n",
    "            \"Mutable\",\n",
    "            \"Lazy Evaluations\",\n",
    "            \"Fault Tolerant\"\n",
    "        ],\n",
    "        \"answer_idx\": [0, 2, 3]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"If I have an RDD with 1.000.000.000 elements, should I use the 'collect()' function?\",\n",
    "        \"options\": [\n",
    "            \"Yes\",\n",
    "            \"No\"\n",
    "        ],\n",
    "        \"answer_idx\": 1\n",
    "    }\n",
    "]\n",
    "\n",
    "question_widgets = []\n",
    "for i, q in enumerate(questions):\n",
    "    question_widgets += create_question(i+1, q[\"question\"], q[\"options\"], q[\"answer_idx\"])\n",
    "\n",
    "# Display the widgets\n",
    "display(*question_widgets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff303ae-e034-400d-98d3-72da6fe5cd56",
   "metadata": {},
   "source": [
    "## Code Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5eeb90-b80d-4d33-a9e0-581a67a672f3",
   "metadata": {},
   "source": [
    "**Exercise 1: Create a SparkSession**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa03f12-673a-4de1-87c2-190bf1f5b9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91cddb7-2944-4d9e-a799-521075687ef2",
   "metadata": {},
   "source": [
    "**Exercise 2 Load the provided list of Strings into an RDD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dc9517-2e16-42c1-806c-fc8f82809937",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_list = [\"Spark is such a cool piece of software\", \"I love Python\", \"The MapReduce model was revolutionary\", \"I like dogs\"]\n",
    "\n",
    "# Load list into RDD\n",
    "rdd = ???"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
