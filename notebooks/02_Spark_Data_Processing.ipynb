{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6daec47-e879-4cc4-a7fa-ea735ee23d42",
   "metadata": {},
   "source": [
    "# Spark Data Processing\n",
    "\n",
    "Spark has two different types of operations Transformations and Actions.\n",
    "\n",
    "## Transformations \n",
    "- Operations that create a new RDD, usually based on a previous one. \n",
    "- Does not evaluate the expression until an action is called (lazy evaluation).\n",
    "- Spark is able to infer the output type.\n",
    "- You can concatenate multiple transformations, before an action.\n",
    "    \n",
    "There are two types of transformations:\n",
    "\n",
    "<img src=\"images/transformation_types.png\" title=\"Spark transformation types\" width=\"700px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a69d376-6371-4b54-ad60-f5df4e42d55a",
   "metadata": {},
   "source": [
    "### **Narrow Transformations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba1e3bb-9de9-4c04-88da-cba5b5305ec9",
   "metadata": {},
   "source": [
    "Some examples of narrow transformations are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f710e584-04cd-40ff-8734-4670157c0066",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Map**\n",
    "\n",
    "A given function is applied for each pair of key-value to generate an intermediate key-value. You can combine multiple Map functions.\n",
    "\n",
    "**In simple terms**\n",
    "\n",
    "Imagine you have a list of items, and for each item, you want to perform a specific operation or transformation. The \\\"map\\\" operation takes each item in the list, applies a function to it, and produces a new list with the transformed items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b74ef6-dff3-418e-96a1-73407716450f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "?rdd.map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41034b5-d124-423d-b63b-cbb7743cb216",
   "metadata": {},
   "source": [
    "**Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0daf12a-db3c-44fc-b55c-3f8aabe583dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doubled_rdd = rdd.map(lambda x: x * 2)\n",
    "doubled_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20acaedb-7dd5-485e-ad87-aa1aa40da8bd",
   "metadata": {},
   "source": [
    "#### **Filter**\n",
    "Return a new RDD containing only the elements that satisfy a predicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb402cc8-5667-456a-bf78-2c5b03db6086",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "?rdd.filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6f9305-f7e1-4460-aeb0-cc0457df37c7",
   "metadata": {},
   "source": [
    "**Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b1843a-4ffa-45cf-89f9-fdacf95817c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "even_rdd = rdd.filter(lambda x: x % 2 == 0)\n",
    "even_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e2b517-7e48-4ec0-aca6-23d1998ed9b1",
   "metadata": {},
   "source": [
    "### **Wide Transformations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7f86f1-8b3e-49c9-b829-f23d29d2f283",
   "metadata": {},
   "source": [
    "This transformations need to use data from other partitions and, thus, perform a Shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e951001c-c144-4965-aae7-29be8edb19dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **The Shuffle Process**\n",
    "\n",
    "It's the process of redistributing the data across the partitions. This can involve reorganizing the data within a single machine or moving data across multiple machines in the cluster. Shuffling can be expensive in terms of performance because it involves disk I/O, data serialization, and network I/O.\n",
    "\n",
    "**In simple terms**\n",
    "\n",
    "Imagine you're a school teacher, and you've given each student in your class a card with a number on it. Initially, the students are seated randomly. Now, you want to group them based on the number they hold, say, all students holding even numbers on one side and those with odd numbers on the other.\n",
    "\n",
    "To do this, students would have to get up, move around, and find their new positions based on their card numbers. This process of rearranging is analogous to the \"shuffle\" operation in Spark.\n",
    "\n",
    "**Why does shuffle happen?**\n",
    "\n",
    "Shuffle usually occurs during operations that require data reorganization. Common operations in Spark that cause shuffling include:\n",
    "\n",
    "- **groupBy**: Grouping data by certain keys.\n",
    "- **reduceByKey**: Reducing data by key.\n",
    "- **join**: Joining two datasets based on keys.\n",
    "\n",
    "**Why is shuffle important?**\n",
    "\n",
    "Understanding shuffle is essential because:\n",
    "\n",
    "- **Performance Implications**: Shuffling can be a performance bottleneck. It involves writing data to disk, transferring data over the network, and reading data back into memory. If you're aware of when shuffling occurs, you can potentially optimize your Spark jobs to minimize shuffling.\n",
    "\n",
    "- **Resource Management**: Shuffling can consume a significant amount of resources. Knowing when and why shuffling is happening can help in tuning the Spark configuration and resources appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3662a2d9-0b89-427f-9aa5-a770527d9606",
   "metadata": {},
   "source": [
    "Some examples of wide transformations are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226fb5c4-381c-46b5-b88e-2dab739f6a4b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **GroupBy**\n",
    "Groups items by a condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e567c0-715c-43a8-9c0e-cd72e83163e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "?rdd.groupBy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c7155c-d4f2-4630-8a27-516749ac1047",
   "metadata": {},
   "source": [
    "**Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5b4182-8e02-4cde-a53f-9c42e2cd6e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "even_odd_groups_rdd = rdd.groupBy(lambda x: x % 2 == 0)\n",
    "[[elem[0], list(elem[1])] for elem in even_odd_groups_rdd.collect()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a94bd9-d7d3-4edf-9861-2bdc5ed4f441",
   "metadata": {},
   "source": [
    "#### **Repartition**\n",
    "Rearranges the RDD to match the new number of partitions with equal size of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24847a62-32db-4208-88e3-0185d6fc548e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "?rdd.repartition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ca63e6-d36c-41b9-8b6e-8ccce7b853b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3336da2e-4323-4587-8296-6236135f337e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rdd_1_partitions = rdd.repartition(1)\n",
    "rdd_3_partitions = rdd.repartition(3)\n",
    "print(rdd.glom().collect())\n",
    "print(rdd_1_partitions.glom().collect())\n",
    "print(rdd_3_partitions.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d33b0ce-c716-48db-ab63-7ccd75bc9470",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Coalesce**\n",
    "Rearranges the RDD to match the new number of partitions with equal size of partitions. If `shuffle` is `False`, you can only reduce the number of partitions and the transformation will be **narrow**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3399eab-7a26-4426-9d80-220032dba1b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "?rdd.coalesce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33acb99-2b26-434a-889c-1de2e3a8fe3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646f4492-7180-4978-bb6a-b4d98c09be61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rdd_1_partitions = rdd.coalesce(1)\n",
    "rdd_3_partitions = rdd.coalesce(3)\n",
    "print(rdd.glom().collect())\n",
    "print(rdd_1_partitions.glom().collect())\n",
    "print(rdd_3_partitions.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6bc20e-c8a7-41ec-a59a-eb051fe3c40e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Actions\n",
    "- Operations that evaluates all the transformations defined.\n",
    "- Forces the evaluation to save or use the result data.\n",
    "\n",
    "Some examples of actions are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6c82a1-c8cf-4d5a-adcc-90b018ab6f3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Reduce**\n",
    "\n",
    "A combination function that groups each key to calculate the aggregation of the multiple values associated to the key.\n",
    "\n",
    "**In simple terms**\n",
    "\n",
    "After you've transformed your list using \"map\", you might want to combine these items in some way to produce a single result. That's where \"reduce\" comes in. \"Reduce\" takes the list and applies a function that combines two items at a time, repeatedly, until only one item (a single result) remains.\n",
    "\n",
    "It's like folding a long piece of paper: you take two adjacent sections, fold them together, then fold the resulting piece with the next section, and so on, until you're left with a small, folded chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e1988b-70d5-4acb-b8a8-8d090ee0214b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "?rdd.reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8800bc3-a0f9-4dd9-a791-1f0401242a3d",
   "metadata": {},
   "source": [
    "**Example**: Using the list of doubled numbers from before, \\\\([2, 4, 6, 8, 10]\\\\), let's say you want to find their sum. Using a \\\"reduce\\\" operation, you'd combine two numbers at a time until you get the total sum: \\\\(2 + 4 = 6\\\\), \\\\(6 + 6 = 12\\\\), \\\\(12 + 8 = 20\\\\), and \\\\(20 + 10 = 30\\\\). The final result is \\\\(30\\\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1d122c-e424-4c68-9b3d-208adf9df5a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add(x, y):\n",
    "    res = x + y\n",
    "    print(f'{x} + {y} = {res}')\n",
    "    return res\n",
    "\n",
    "sum_result = doubled_rdd.reduce(add)\n",
    "sum_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f2ae46-b057-4c7b-bc25-54e3e76ab665",
   "metadata": {},
   "source": [
    "#### **Fold**\n",
    "\n",
    "Applies the reduction function combining elements together, but including a zero value for each reduction step (partition=.\n",
    "\n",
    "**In simple terms**\n",
    "\n",
    "Is the same as the Reduce function but also recieves an initial value called `zeroValue`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007d8479-c693-45e7-8975-0cd987a2b849",
   "metadata": {},
   "outputs": [],
   "source": [
    "?rdd.fold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5861df7-59f9-4e16-9330-45f83175885f",
   "metadata": {},
   "source": [
    "**Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c49eb2f-6942-497d-b5ee-eab3b74e4e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "doubled_rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bfda90-c876-4c57-9a6e-a4b88bdee167",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_result = doubled_rdd.fold(3, add)\n",
    "fold_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507fcf4b-6b03-45a9-8dde-ee092758a88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_result = doubled_rdd.coalesce(1).fold(3, add)\n",
    "fold_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aff4bfd-c2ec-4c70-bc25-1f42a5709c39",
   "metadata": {},
   "source": [
    "In the following image you can see the process of counting words for a list of sentences performing Map, Shuffle and Reduce.\n",
    "\n",
    "<img src=\"images/map_reduce_shuffle_operation.png\" title=\"Map, Reduce and Shuffle Operation\" width=\"700px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167cc179-7d75-459d-a095-7e435b59f22f",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdda2be-c08f-4f26-ae6e-b4e9d5c76bb3",
   "metadata": {},
   "source": [
    "1. Create a SparkSession\n",
    "2. Load the provided list of Strings into an RDD\n",
    "3. Count the words in each sentence\n",
    "4. Filter the sentences with less than 5 words\n",
    "5. Print the sum of all the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f95e831-f804-41b2-877c-c62fee7933b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_list = [\"Spark is such a cool piece of software\", \"I love Python\", \"The MapReduce model was revolutionary\", \"I like dogs\"]\n",
    "\n",
    "# ---- INSERT CODE HERE ----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da9dbb7-f5a7-4f53-bd55-7a95f84c0e65",
   "metadata": {},
   "source": [
    "**Solution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1560900-5fc5-4a4f-a346-74afcfb129b6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "str_list = [\"Spark is such a cool piece of software\", \"I love Python\", \"The MapReduce model was revolutionary\", \"I like dogs\"]\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(str_list)\n",
    "word_count_rdd = rdd.map(lambda s: len(s.split()))\n",
    "filtered_word_count_rdd = word_count_rdd.filter(lambda x: x >= 5)\n",
    "words_sum = filtered_word_count_rdd.reduce(lambda n1, n2: n1 + n2)\n",
    "words_sum"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
